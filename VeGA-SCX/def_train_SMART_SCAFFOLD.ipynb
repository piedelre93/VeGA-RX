{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac26b590-8284-4c5d-92e3-d550c7bf33b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/grad/Desktop/pietro/denovo/2/xx\n"
     ]
    }
   ],
   "source": [
    "cd /home/grad/Desktop/pietro/denovo/2/xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f449d892-de2f-4591-a0ff-45d89be03eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# [ 1. CONFIGURAZIONE ]\n",
    "# ====================================================================\n",
    "\n",
    "class Config:\n",
    "    # Percorsi file\n",
    "    SMILES_FILE = \"/home/grad/Desktop/pietro/denovo/2/compacted_smiles.txt\"\n",
    "    SMARTS_RX_FILE = \"/home/grad/Desktop/pietro/denovo/2/SMART_RX/smartsrx-main/smartsrx.json\"\n",
    "    TRAIN_DATA_PATH = \"/home/grad/Desktop/pietro/denovo/2/train_data.pkl\"\n",
    "    VAL_DATA_PATH = \"/home/grad/Desktop/pietro/denovo/2/val_data.pkl\"\n",
    "    VOCAB_PATH = \"/home/grad/Desktop/pietro/denovo/2/vocab.json\"\n",
    "    # Iperparametri Modello\n",
    "    EMBED_DIM = 512\n",
    "    TRANSFORMER_LAYERS = 6\n",
    "    TRANSFORMER_HEADS = 6\n",
    "    FF_DIM = 2048\n",
    "    DROPOUT_RATE = 0.10\n",
    "    L2_REG = 1e-4\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 48\n",
    "    EPOCHS = 200\n",
    "    MAX_LENGTH = 140         # Aumentato leggermente per ospitare 3 parti\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    VALID_RATIO = 0.1\n",
    "    \n",
    "    # Curriculum & Augmentation\n",
    "    CURRICULUM_START_COMPLEXITY = 10\n",
    "    CURRICULUM_COMPLEXITY_STEP = 5\n",
    "    LOSS_STABILITY_THRESHOLD = 0.01\n",
    "    WARMUP_EPOCHS = 5\n",
    "    AUGMENT_PROB = 0.1\n",
    "    \n",
    "    # Generazione\n",
    "    TEMPERATURE = 1.0\n",
    "    GEN_NUM = 10\n",
    "    PRINT_EVERY = 100\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0de9d2b-7a57-4526-b238-c78b11bd4cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 11:13:55.956180: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-16 11:13:55.970245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771236835.987294 1229876 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771236835.992639 1229876 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771236836.005599 1229876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771236836.005619 1229876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771236836.005620 1229876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771236836.005621 1229876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-16 11:13:56.010489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from threading import Lock\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# RDKit Imports\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.Chem import FilterCatalog  # Necessario per SMARTS-RX\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow Imports\n",
    "from tensorflow.keras.layers import Layer, Embedding, Input, LayerNormalization, MultiHeadAttention, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "# Configurazione Logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "# ====================================================================\n",
    "# [ 2. GESTIONE SMARTS-RX & SCAFFOLD ]\n",
    "# ====================================================================\n",
    "\n",
    "# Variabili Globali Catalogo\n",
    "SMARTS_CATALOG = None\n",
    "\n",
    "def initialize_smarts_catalog():\n",
    "    \"\"\"Carica il catalogo SMARTS-RX.\"\"\"\n",
    "    global SMARTS_CATALOG\n",
    "    if SMARTS_CATALOG is not None: return\n",
    "\n",
    "    try:\n",
    "        with open(config.SMARTS_RX_FILE, \"rt\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Errore caricamento SMARTS JSON: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    catalog = FilterCatalog.FilterCatalog()\n",
    "    count = 0\n",
    "    for entry in data.get(\"data\", []):\n",
    "        name = entry.get(\"specific_type\")\n",
    "        smarts = entry.get(\"smarts\")\n",
    "        if name and smarts:\n",
    "            pattern = Chem.MolFromSmarts(smarts)\n",
    "            if pattern:\n",
    "                catalog.AddEntry(FilterCatalog.FilterCatalogEntry(name, FilterCatalog.SmartsMatcher(pattern)))\n",
    "                count += 1\n",
    "    SMARTS_CATALOG = catalog\n",
    "    logger.info(f\"Catalogo SMARTS-RX caricato: {count} regole.\")\n",
    "\n",
    "def get_smarts_fingerprint(mol: Chem.Mol) -> List[str]:\n",
    "    \"\"\"Restituisce lista nomi gruppi funzionali presenti.\"\"\"\n",
    "    if not mol or SMARTS_CATALOG is None: return []\n",
    "    matches = SMARTS_CATALOG.GetMatches(mol)\n",
    "    # Set per unicitÃ , sorted per ordine deterministico\n",
    "    return sorted(list(set([m.GetDescription() for m in matches])))\n",
    "\n",
    "def get_murcko_scaffold_tokens(mol: Chem.Mol) -> List[str]:\n",
    "    \"\"\"Estrae scaffold e lo tokenizza.\"\"\"\n",
    "    if not mol: return []\n",
    "    try:\n",
    "        scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "        smi = Chem.MolToSmiles(scaffold, canonical=True, isomericSmiles=False)\n",
    "        if not smi: return []\n",
    "        return robust_tokenize(smi)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# ====================================================================\n",
    "# [ 3. HELPER CHIMICI BASE ]\n",
    "# ====================================================================\n",
    "\n",
    "def robust_tokenize(smiles: str) -> list:\n",
    "    pattern = (\n",
    "        r\"(\\[[^\\[\\]]{1,6}\\]|\"                 # atomi in parentesi quadre\n",
    "        r\"Br|Cl|Si|Na|Mg|Mn|Ca|Fe|Zn|Se|Li|K|Al|B|\"  # elementi multi-char\n",
    "        r\"R[0-9]|r[0-9]|a[0-9]|\"             # ring labels\n",
    "        r\"[A-Za-z0-9@+\\-\\\\\\/\\(\\)=#\\$\\.\\%,])\"  # singoli caratteri, incluso '%'\n",
    "    )\n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    stack = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('['): stack.append(t)\n",
    "        if t.endswith(']'):\n",
    "            if not stack: return []\n",
    "            stack.pop()\n",
    "    return tokens\n",
    "\n",
    "def validate_and_fix_smiles(smiles: str) -> str:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is None: return None\n",
    "        try: Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        except: pass\n",
    "        return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "    except: return None\n",
    "\n",
    "def randomize_smiles(smiles: str, num_versions: int = 1) -> List[str]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return []\n",
    "    res = []\n",
    "    try:\n",
    "        s = Chem.MolToSmiles(mol, doRandom=True, canonical=False)\n",
    "        if s: res.append(s)\n",
    "    except: pass\n",
    "    return res\n",
    "\n",
    "def compute_complexity_from_tokens(tokens: List[str]) -> int:\n",
    "    smiles = ''.join(tokens)\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol: return 999\n",
    "        return Chem.GetSSSR(mol) + smiles.count('(')\n",
    "    except: return 999\n",
    "\n",
    "# ====================================================================\n",
    "# [ 4. PREPROCESSING DATASET (IBRIDO) ]\n",
    "# ====================================================================\n",
    "\n",
    "def process_dataset(data: List[str]) -> Tuple[List[Tuple[List[str], List[str], List[str]]], List[str], int]:\n",
    "    \"\"\"\n",
    "    Output Tuple: (SMARTS_TOKENS, SCAFFOLD_TOKENS, TARGET_TOKENS)\n",
    "    \"\"\"\n",
    "    initialize_smarts_catalog()\n",
    "    \n",
    "    processed = []\n",
    "    all_tokens = set()\n",
    "    # Abbiamo 2 separatori ora: SMARTS|SEP|SCAFFOLD|SEP|TARGET\n",
    "    # Token speciali totali nella sequenza: START, SEP, SEP, END = 4 token logici (ma SEP Ã¨ lo stesso ID)\n",
    "    \n",
    "    logger.info(\"Inizio Preprocessing Ibrido (SMARTS + Scaffold)...\")\n",
    "    \n",
    "    for idx, s in enumerate(data):\n",
    "        if idx % 10000 == 0: logger.info(f\"Processati {idx}/{len(data)}\")\n",
    "            \n",
    "        fixed = validate_and_fix_smiles(s)\n",
    "        if not fixed: continue\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(fixed)\n",
    "        if not mol: continue\n",
    "\n",
    "        # 1. Target (SMILES)\n",
    "        tokens_target = robust_tokenize(fixed)\n",
    "        if not tokens_target: continue\n",
    "        \n",
    "        # 2. Scaffold (Murcko)\n",
    "        tokens_scaffold = get_murcko_scaffold_tokens(mol)\n",
    "        \n",
    "        # 3. SMARTS-RX\n",
    "        tokens_smarts = get_smarts_fingerprint(mol)\n",
    "        \n",
    "        # Calcolo Lunghezza: START + SMARTS + SEP + SCAFFOLD + SEP + TARGET + END\n",
    "        total_len = 1 + len(tokens_smarts) + 1 + len(tokens_scaffold) + 1 + len(tokens_target) + 1\n",
    "\n",
    "        if 5 <= total_len <= config.MAX_LENGTH:\n",
    "            processed.append((tokens_smarts, tokens_scaffold, tokens_target))\n",
    "            all_tokens.update(tokens_target)\n",
    "            all_tokens.update(tokens_scaffold)\n",
    "            all_tokens.update(tokens_smarts) # Aggiunge nomi come \"Alcohol\", \"Ketone\"\n",
    "\n",
    "    # Costruzione Vocabolario\n",
    "    vocab = ['<PAD>', '<START>', '<END>', '<SEP>', '<UNK>']\n",
    "    vocab.extend(sorted(list(all_tokens)))\n",
    "    \n",
    "    # Ricalcolo Max Len\n",
    "    lengths = []\n",
    "    for t_smart, t_scaf, t_targ in processed:\n",
    "        lengths.append(1 + len(t_smart) + 1 + len(t_scaf) + 1 + len(t_targ) + 1)\n",
    "        \n",
    "    if processed:\n",
    "        max_len = min(int(np.percentile(lengths, 99)), config.MAX_LENGTH)\n",
    "    else:\n",
    "        max_len = config.MAX_LENGTH\n",
    "    \n",
    "    return processed, vocab, max_len\n",
    "\n",
    "# ====================================================================\n",
    "# [ 5. CLASSI THREAD-SAFE ]\n",
    "# ====================================================================\n",
    "\n",
    "class ThreadSafeIterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = Lock()\n",
    "    def __iter__(self): return self\n",
    "    def __next__(self):\n",
    "        with self.lock: return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    def wrapper(*args, **kwargs): return ThreadSafeIterator(func(*args, **kwargs))\n",
    "    return wrapper\n",
    "\n",
    "# ====================================================================\n",
    "# [ 6. GENERATORE CURRICULUM (3 PARTI) ]\n",
    "# ====================================================================\n",
    "\n",
    "class CurriculumSmilesGenerator:\n",
    "    def __init__(self, processed_data, vocab: List[str]):\n",
    "        self.char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        self.original_data = []\n",
    "        \n",
    "        # data = (smarts, scaffold, target)\n",
    "        for smarts, scaffold, target in processed_data:\n",
    "            comp = compute_complexity_from_tokens(target)\n",
    "            self.original_data.append(((smarts, scaffold, target), comp))\n",
    "            \n",
    "        valid_comps = [c for _, c in self.original_data if c != 999]\n",
    "        self.max_complexity = max(valid_comps) if valid_comps else 0\n",
    "        self.current_complexity = config.CURRICULUM_START_COMPLEXITY\n",
    "        self.available_data = self._filter_data()\n",
    "        # Per novelty check\n",
    "        self.train_smiles = {''.join(t) for (_, _, t), _ in self.original_data}\n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def _filter_data(self):\n",
    "        filtered = [dp for dp, c in self.original_data if c <= self.current_complexity]\n",
    "        return filtered if filtered else [dp for dp, _ in self.original_data]\n",
    "    \n",
    "    def update_complexity(self, epoch: int, loss_diff: float = None):\n",
    "        with self.lock:\n",
    "            if loss_diff is not None and loss_diff < config.LOSS_STABILITY_THRESHOLD:\n",
    "                self.current_complexity = min(self.current_complexity + config.CURRICULUM_COMPLEXITY_STEP, self.max_complexity)\n",
    "            else:\n",
    "                if epoch <= config.WARMUP_EPOCHS:\n",
    "                    incr = int((self.max_complexity - config.CURRICULUM_START_COMPLEXITY) * (epoch / config.WARMUP_EPOCHS))\n",
    "                    self.current_complexity = config.CURRICULUM_START_COMPLEXITY + incr\n",
    "                else:\n",
    "                    self.current_complexity = self.max_complexity\n",
    "            \n",
    "            self.available_data = self._filter_data()\n",
    "            if not self.available_data:\n",
    "                self.available_data = [dp for dp, _ in self.original_data]\n",
    "                logger.warning(\"Reset available_data (fallback).\")\n",
    "    \n",
    "    @threadsafe_generator\n",
    "    def __call__(self):\n",
    "        PAD_IDX = self.char2idx['<PAD>']\n",
    "        UNK_IDX = self.char2idx.get('<UNK>', PAD_IDX)\n",
    "\n",
    "        while True:\n",
    "            inputs = np.full((config.BATCH_SIZE, config.MAX_LENGTH), PAD_IDX, dtype=np.int32)\n",
    "            targets = np.full_like(inputs, PAD_IDX)\n",
    "            \n",
    "            for i in range(config.BATCH_SIZE):\n",
    "                with self.lock:\n",
    "                    try:\n",
    "                        data_pair = random.choice(self.available_data)\n",
    "                    except:\n",
    "                        self.available_data = [dp for dp, _ in self.original_data]\n",
    "                        data_pair = random.choice(self.available_data)\n",
    "                \n",
    "                # Unpacking 3 parti\n",
    "                t_smarts, t_scaf, t_targ = data_pair\n",
    "\n",
    "                # Augmentation solo sul target\n",
    "                curr_target = t_targ\n",
    "                if random.random() < config.AUGMENT_PROB:\n",
    "                    try:\n",
    "                        aug = randomize_smiles(''.join(t_targ), 1)\n",
    "                        if aug:\n",
    "                            tok = robust_tokenize(aug[0])\n",
    "                            if tok: curr_target = tok\n",
    "                    except: pass\n",
    "\n",
    "                # STRUTTURA: [START] SMARTS [SEP] SCAFFOLD [SEP] TARGET [END]\n",
    "                seq = (['<START>'] + \n",
    "                       t_smarts + ['<SEP>'] + \n",
    "                       t_scaf + ['<SEP>'] + \n",
    "                       curr_target + ['<END>'])\n",
    "                \n",
    "                padded_seq = (seq + ['<PAD>'] * config.MAX_LENGTH)[:config.MAX_LENGTH]\n",
    "                indices = [self.char2idx.get(t, UNK_IDX) for t in padded_seq]\n",
    "                \n",
    "                inputs[i] = indices\n",
    "                targets[i, :-1] = inputs[i][1:]\n",
    "                targets[i, -1] = PAD_IDX\n",
    "                \n",
    "            yield inputs, targets\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            self.__call__,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32)\n",
    "            )\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ====================================================================\n",
    "# [ 7. MODELLO TRANSFORMER ]\n",
    "# ====================================================================\n",
    "\n",
    "# ====================================================================\n",
    "# [ 7. ARCHITETTURA MODELLO & LOSS ]\n",
    "# ====================================================================\n",
    "\n",
    "# 1. DEFINIZIONE LOSS GLOBALE (Necessario per il salvataggio/caricamento)\n",
    "def smoothed_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcola la Cross Entropy ignorando i token di padding (0).\n",
    "    \"\"\"\n",
    "    # Cast a int per le labels\n",
    "    y_true_int = tf.cast(y_true, tf.int32)\n",
    "    # Maschera: 1 dove c'Ã¨ un token reale, 0 dove c'Ã¨ padding\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "    \n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_int, logits=y_pred)\n",
    "    \n",
    "    # Media pesata (somma loss valida / numero token validi)\n",
    "    return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "# 2. LAYERS CUSTOM\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Usiamo la lunghezza massima definita nella config per pre-calcolare la matrice\n",
    "        max_len = config.MAX_LENGTH\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        \n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        \n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Taglia la matrice posizionale alla lunghezza attuale del batch\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"embed_dim\": self.embed_dim}\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim, \n",
    "            kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\", kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)),\n",
    "            Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG))\n",
    "        ])\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.d1 = Dropout(rate)\n",
    "        self.d2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        # Creazione Maschera Causale (impedisce di guardare il futuro)\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        \n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        out1 = self.ln1(inputs + self.d1(attn_output, training=training))\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.ln2(out1 + self.d2(ffn_output, training=training))\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"embed_dim\": self.embed_dim, \"num_heads\": self.num_heads, \"ffn_dim\": self.ffn_dim, \"rate\": self.rate}\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super().__init__()\n",
    "        # FIX: Usa tf.cast per evitare errori di tipo\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": float(self.embed_dim), \"warmup_steps\": float(self.warmup_steps)}\n",
    "\n",
    "# 3. COSTRUZIONE MODELLO\n",
    "def build_improved_model(vocab_size: int) -> Model:\n",
    "    inputs = Input(shape=(config.MAX_LENGTH,))\n",
    "    \n",
    "    # FIX: mask_zero=False perchÃ© gestiamo tutto manualmente (causal mask + loss)\n",
    "    x = Embedding(vocab_size, config.EMBED_DIM, mask_zero=False)(inputs)\n",
    "    \n",
    "    x = DynamicPositionalEncoding(config.EMBED_DIM)(x)\n",
    "    x = Dropout(config.DROPOUT_RATE)(x)\n",
    "    \n",
    "    for _ in range(config.TRANSFORMER_LAYERS):\n",
    "        x = ImprovedTransformerBlock(\n",
    "            config.EMBED_DIM, \n",
    "            config.TRANSFORMER_HEADS, \n",
    "            config.FF_DIM, \n",
    "            rate=config.DROPOUT_RATE\n",
    "        )(x)\n",
    "        \n",
    "    outputs = Dense(vocab_size)(x)\n",
    "    \n",
    "    # Setup Optimizer & Compile\n",
    "    opt = tf.keras.optimizers.Adam(\n",
    "        learning_rate=CustomSchedule(config.EMBED_DIM), \n",
    "        clipnorm=config.GRADIENT_CLIP\n",
    "    )\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    # Usiamo la loss globale definita sopra\n",
    "    model.compile(optimizer=opt, loss=smoothed_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ====================================================================\n",
    "# [ 8. MONITORAGGIO (Parsing Complesso) ]\n",
    "# ====================================================================\n",
    "\n",
    "class CustomTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}; lr = self.model.optimizer.learning_rate\n",
    "        logs['lr'] = lr(epoch).numpy() if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule) else lr.numpy()\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class EnhancedTrainingMonitor(Callback):\n",
    "    def __init__(self, val_gen: CurriculumSmilesGenerator):\n",
    "        super().__init__()\n",
    "        self.val_gen = val_gen\n",
    "        self.best_val_loss = np.inf; self.prev_val_loss = None\n",
    "\n",
    "    def generate_sample(self, num: int):\n",
    "        generated, valid = [], []\n",
    "        PAD, START, END, SEP = [self.val_gen.char2idx[k] for k in ['<PAD>','<START>','<END>','<SEP>']]\n",
    "        input_seq = np.full((1, config.MAX_LENGTH), PAD, dtype=np.int32)\n",
    "        \n",
    "        for _ in range(num):\n",
    "            input_seq.fill(PAD); input_seq[0, 0] = START\n",
    "            for t in range(1, config.MAX_LENGTH):\n",
    "                logits = self.model(input_seq, training=False)[0, t-1]\n",
    "                probs = tf.nn.softmax(logits / config.TEMPERATURE).numpy()\n",
    "                if np.sum(probs) < 1e-6: break\n",
    "                sampled = np.random.choice(len(probs), p=probs)\n",
    "                input_seq[0, t] = sampled\n",
    "                if sampled == END: break\n",
    "            \n",
    "            indices = input_seq[0].tolist()\n",
    "            raw_tokens = [self.val_gen.idx2char[i] for i in indices if i not in {PAD, START}]\n",
    "            \n",
    "            # Parsing Logic: START [SMARTS] SEP [SCAFFOLD] SEP [TARGET] END\n",
    "            # Dobbiamo trovare l'ultimo SEP per ottenere il target\n",
    "            smi_str = \"\"\n",
    "            try:\n",
    "                # Conta quanti SEP ci sono\n",
    "                sep_indices = [i for i, x in enumerate(raw_tokens) if x == '<SEP>']\n",
    "                if len(sep_indices) >= 2:\n",
    "                    # Prendi tutto dopo il SECONDO SEP\n",
    "                    target_tokens = raw_tokens[sep_indices[1]+1:]\n",
    "                    smi_str = \"\".join(target_tokens).split('<END>')[0]\n",
    "                elif len(sep_indices) == 1:\n",
    "                    # Fallback: forse ha saltato uno step\n",
    "                    target_tokens = raw_tokens[sep_indices[0]+1:]\n",
    "                    smi_str = \"\".join(target_tokens).split('<END>')[0]\n",
    "                else:\n",
    "                    smi_str = \"\".join(raw_tokens).split('<END>')[0]\n",
    "            except: pass\n",
    "            \n",
    "            final = validate_and_fix_smiles(smi_str)\n",
    "            if final: valid.append(final)\n",
    "            generated.append(final or smi_str)\n",
    "            \n",
    "        return generated, valid\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if self.prev_val_loss and 'val_loss' in logs:\n",
    "            diff = (self.prev_val_loss - logs['val_loss']) / self.prev_val_loss\n",
    "            self.val_gen.update_complexity(epoch, diff)\n",
    "        if logs.get('val_loss', np.inf) < self.best_val_loss: self.best_val_loss = logs['val_loss']\n",
    "        self.prev_val_loss = logs.get('val_loss')\n",
    "        \n",
    "        if (epoch + 1) % config.PRINT_EVERY == 0:\n",
    "            gen, val = self.generate_sample(config.GEN_NUM)\n",
    "            validity = len(val) / config.GEN_NUM\n",
    "            novel = len([s for s in val if s not in self.val_gen.train_smiles])\n",
    "            logger.info(f\"\\nEPOCH {epoch+1}: Validity {validity:.1%} | Novelty {novel}/{len(val)}\")\n",
    "            if val: logger.info(f\"Sample: {val[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb543521-7b5b-40e9-bbb9-76c2ca15a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 11:14:07,078 [INFO] ğŸš€ START TRAINING: MULTI-CONDITIONAL (SMARTS + SCAFFOLD -> TARGET)\n",
      "2026-02-16 11:14:07,324 [INFO] Caricati 891004 SMILES.\n",
      "2026-02-16 11:14:07,409 [INFO] Catalogo SMARTS-RX caricato: 406 regole.\n",
      "2026-02-16 11:14:07,409 [INFO] Inizio Preprocessing Ibrido (SMARTS + Scaffold)...\n",
      "2026-02-16 11:14:07,410 [INFO] Processati 0/891004\n",
      "[11:14:07] SMILES Parse Error: syntax error while parsing: SMILES\n",
      "[11:14:07] SMILES Parse Error: Failed parsing SMILES 'SMILES' for input: 'SMILES'\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# [ 9. MAIN EXECUTION ]\n",
    "# ====================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"ğŸš€ START TRAINING: MULTI-CONDITIONAL (SMARTS + SCAFFOLD -> TARGET)\")\n",
    "    \n",
    "    # 1. Verifica Files\n",
    "    if not os.path.exists(config.SMILES_FILE):\n",
    "        logger.error(\"File SMILES non trovato.\"); sys.exit(1)\n",
    "    if not os.path.exists(config.SMARTS_RX_FILE):\n",
    "        logger.error(\"File SMARTS JSON non trovato.\"); sys.exit(1)\n",
    "\n",
    "    with open(config.SMILES_FILE) as f:\n",
    "        raw_smiles = [line.strip() for line in f if line.strip()]\n",
    "    logger.info(f\"Caricati {len(raw_smiles)} SMILES.\")\n",
    "\n",
    "    # 2. Preprocessing\n",
    "    processed, vocab, max_len = process_dataset(raw_smiles)\n",
    "    config.MAX_LENGTH = max_len\n",
    "    logger.info(f\"Vocab: {len(vocab)} | MaxLen: {max_len}\")\n",
    "\n",
    "    # 3. Salvataggi\n",
    "    with open(\"vocab.json\", \"w\") as f: json.dump(vocab, f)\n",
    "    with open(\"char2idx.pkl\", \"wb\") as f: pickle.dump({c:i for i,c in enumerate(vocab)}, f)\n",
    "    with open(\"idx2char.pkl\", \"wb\") as f: pickle.dump({i:c for i,c in enumerate(vocab)}, f)\n",
    "\n",
    "    # 4. Split (stratificato su target)\n",
    "    stratify = [min(len(t[2]), 20) for t in processed]\n",
    "    try:\n",
    "        train_data, val_data = train_test_split(processed, test_size=config.VALID_RATIO, stratify=stratify, random_state=42)\n",
    "    except:\n",
    "        train_data, val_data = train_test_split(processed, test_size=config.VALID_RATIO, random_state=42)\n",
    "\n",
    "    # 5. Esempio di Log\n",
    "    if train_data:\n",
    "        s_smart, s_scaf, s_targ = train_data[0]\n",
    "        logger.info(f\"\\n--- ESEMPIO SEQ ---\")\n",
    "        logger.info(f\"SMARTS:   {s_smart}\")\n",
    "        logger.info(f\"SCAFFOLD: {''.join(s_scaf)}\")\n",
    "        logger.info(f\"TARGET:   {''.join(s_targ)}\")\n",
    "        logger.info(\"-------------------\")\n",
    "\n",
    "    # 6. Avvio\n",
    "    train_gen = CurriculumSmilesGenerator(train_data, vocab)\n",
    "    val_gen = CurriculumSmilesGenerator(val_data, vocab)\n",
    "    \n",
    "    model = build_improved_model(len(vocab))\n",
    "    model.summary()\n",
    "    \n",
    "    callbacks = [\n",
    "        CustomTensorBoard(log_dir=f\"logs/hybrid_{int(time.time())}\"),\n",
    "        EnhancedTrainingMonitor(val_gen),\n",
    "        ModelCheckpoint(\"best_hybrid_model.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        model.fit(\n",
    "            train_gen.get_dataset(),\n",
    "            epochs=config.EPOCHS,\n",
    "            steps_per_epoch=config.STEPS_PER_EPOCH,\n",
    "            validation_data=val_gen.get_dataset(),\n",
    "            validation_steps=max(1, len(val_data)//config.BATCH_SIZE),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        model.save(\"final_hybrid_model.keras\")\n",
    "        logger.info(\"Training Completato.\")\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Training interrotto.\")\n",
    "        model.save(\"interrupted_hybrid.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fce6c5f-a294-42ea-9923-f332edc7aca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/grad/Desktop/pietro/denovo/2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72bf079-f63c-4811-8311-99ab426cfd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ train_data salvato: 801902 molecole\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680d816-e3a4-4683-abda-52a23f913b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b1e79-69f9-45e8-9fc6-df4d3c157e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 08:16:25,181 [INFO] ğŸš€ AVVIO SCRIPT: CARICAMENTO DIRETTO E ADDESTRAMENTO\n",
      "2026-01-17 08:16:25,182 [INFO] ğŸ“‚ Caricamento dati da disco (Pickle)...\n",
      "2026-01-17 08:16:32,331 [INFO] âœ… Dati caricati: 801902 Train, 89101 Val, 370 Vocab\n",
      "2026-01-17 08:16:32,332 [INFO] âš™ï¸ Inizializzazione Generatori...\n",
      "2026-01-17 08:18:46,356 [INFO] ğŸ—ï¸ Costruzione Modello...\n",
      "I0000 00:00:1768634327.314430 3641570 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8888 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">189,440</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dynamic_positional_encoding     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DynamicPositionalEncoding</span>)     â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,502,144</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_1    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,502,144</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_2    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,502,144</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_3    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,502,144</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_4    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,502,144</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_5    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,502,144</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">370</span>)       â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">189,810</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚       \u001b[38;5;34m189,440\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dynamic_positional_encoding     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mDynamicPositionalEncoding\u001b[0m)     â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m10,502,144\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_1    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m10,502,144\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_2    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m10,502,144\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_3    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m10,502,144\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_4    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m10,502,144\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_5    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m10,502,144\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m370\u001b[0m)       â”‚       \u001b[38;5;34m189,810\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,392,114</span> (241.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m63,392,114\u001b[0m (241.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,392,114</span> (241.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m63,392,114\u001b[0m (241.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 08:18:49,267 [INFO] ğŸ”¥ Inizio Training Loop...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768634342.773462 3646777 service.cc:152] XLA service 0x72cd40002f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1768634342.773488 3646777 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A2000 12GB, Compute Capability 8.6\n",
      "2026-01-17 08:19:04.083809: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1768634347.564042 3646777 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2026-01-17 08:19:11.107499: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:11.241825: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 296 bytes spill stores, 296 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:11.675576: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 436 bytes spill stores, 344 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:11.688698: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 572 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:11.961110: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 988 bytes spill stores, 884 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:12.127267: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:12.171275: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:13.042634: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:13.660260: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 828 bytes spill stores, 1616 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:13.839644: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 900 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:14.443228: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 440 bytes spill stores, 344 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:14.472416: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:14.509432: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:14.734952: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:16.095004: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:16.097980: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 348 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:16.168619: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:16.650400: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 96 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:17.111360: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 1188 bytes spill stores, 900 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:17.165409: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 900 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:17.226906: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 812 bytes spill stores, 1632 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:18.183414: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 228 bytes spill stores, 180 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:18.903274: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:19.004943: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 1208 bytes spill stores, 1208 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:19.639367: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 104 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:20.512637: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 224 bytes spill stores, 224 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:20.512996: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 444 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:20.538187: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 540 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:20.679977: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 4600 bytes spill stores, 4520 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:21.015971: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 300 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:21.438173: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 156 bytes spill stores, 156 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:21.765038: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:21.841680: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 432 bytes spill stores, 432 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:21.989958: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 400 bytes spill stores, 400 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:22.040548: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:22.185888: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:22.779487: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:22.905761: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:23.178253: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 676 bytes spill stores, 560 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:23.439761: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 400 bytes spill stores, 400 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:23.698113: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 656 bytes spill stores, 656 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:24.079547: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:24.784197: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:25.027038: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:25.373934: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:25.457358: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 448 bytes spill stores, 448 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:26.104630: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:26.175159: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 420 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:26.219898: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:26.904739: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 4048 bytes spill stores, 4032 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:26.955540: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:26.973148: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 480 bytes spill stores, 480 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:27.260266: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:27.288658: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:27.911028: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:27.937230: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:27.990760: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 656 bytes spill stores, 656 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:28.523603: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:28.646498: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:28.976086: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:29.304868: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 456 bytes spill stores, 456 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:29.914300: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:30.009083: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:30.039331: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:30.390238: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 1784 bytes spill stores, 1752 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:30.410012: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 888 bytes spill stores, 888 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:30.886394: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:31.340787: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:31.394266: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 448 bytes spill stores, 448 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:31.411937: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 888 bytes spill stores, 888 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:31.870836: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:32.285759: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 428 bytes spill stores, 428 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:32.328791: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 1764 bytes spill stores, 1788 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:32.361639: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 456 bytes spill stores, 456 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:32.545515: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 660 bytes spill stores, 660 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:32.655880: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 876 bytes spill stores, 876 bytes spill loads\n",
      "\n",
      "2026-01-17 08:19:32.908834: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 1764 bytes spill stores, 1788 bytes spill loads\n",
      "\n",
      "I0000 00:00:1768634385.811593 3646777 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 689ms/step - loss: 4.2979 - val_loss: 2.8281 - lr: 4.4194e-05\n",
      "Epoch 2/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 693ms/step - loss: 2.7320 - val_loss: 2.2598 - lr: 8.8388e-05\n",
      "Epoch 3/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 693ms/step - loss: 2.1763 - val_loss: 1.7930 - lr: 1.3258e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 705ms/step - loss: 1.7228 - val_loss: 1.3908 - lr: 1.7678e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m694s\u001b[0m 695ms/step - loss: 1.3580 - val_loss: 1.1254 - lr: 2.2097e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m695s\u001b[0m 695ms/step - loss: 1.0903 - val_loss: 0.9329 - lr: 2.6517e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 696ms/step - loss: 0.9280 - val_loss: 0.8389 - lr: 3.0936e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m706s\u001b[0m 707ms/step - loss: 0.8465 - val_loss: 0.7779 - lr: 3.5355e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 693ms/step - loss: 0.8019 - val_loss: 0.7653 - lr: 3.9775e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 692ms/step - loss: 0.7798 - val_loss: 0.7435 - lr: 4.4194e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 692ms/step - loss: 0.7581 - val_loss: 0.7336 - lr: 4.2137e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 692ms/step - loss: 0.7376 - val_loss: 0.7023 - lr: 4.0344e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 688ms/step - loss: 0.7632 - val_loss: 3.1457 - lr: 3.8761e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 689ms/step - loss: 2.9569 - val_loss: 2.7128 - lr: 3.7351e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 688ms/step - loss: 2.7322 - val_loss: 2.7095 - lr: 3.6084e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 687ms/step - loss: 2.6999 - val_loss: 2.6952 - lr: 3.4939e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 689ms/step - loss: 2.6753 - val_loss: 2.6655 - lr: 3.3895e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 688ms/step - loss: 2.7021 - val_loss: 2.6803 - lr: 3.2940e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m 209/1000\u001b[0m \u001b[32mâ”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m6:09\u001b[0m 467ms/step - loss: 2.6699"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"ğŸš€ AVVIO SCRIPT: CARICAMENTO DIRETTO E ADDESTRAMENTO\")\n",
    "    \n",
    "    # 1. CONTROLLO FILE ESISTENTI\n",
    "    if not (os.path.exists(config.TRAIN_DATA_PATH) and \n",
    "            os.path.exists(config.VAL_DATA_PATH) and \n",
    "            os.path.exists(config.VOCAB_PATH)):\n",
    "        logger.error(f\"âŒ File di cache mancanti! Esegui prima lo script di salvataggio.\")\n",
    "        logger.error(f\"   Cercati: {config.TRAIN_DATA_PATH}, {config.VAL_DATA_PATH}, {config.VOCAB_PATH}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 2. CARICAMENTO DATI\n",
    "    logger.info(\"ğŸ“‚ Caricamento dati da disco (Pickle)...\")\n",
    "    try:\n",
    "        with open(config.TRAIN_DATA_PATH, \"rb\") as f: train_data = pickle.load(f)\n",
    "        with open(config.VAL_DATA_PATH, \"rb\") as f: val_data = pickle.load(f)\n",
    "        with open(config.VOCAB_PATH, \"r\") as f: vocab = json.load(f)\n",
    "        logger.info(f\"âœ… Dati caricati: {len(train_data)} Train, {len(val_data)} Val, {len(vocab)} Vocab\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Errore durante il caricamento: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3. SETUP GENERATORI\n",
    "    logger.info(\"âš™ï¸ Inizializzazione Generatori...\")\n",
    "    train_gen = CurriculumSmilesGenerator(train_data, vocab)\n",
    "    val_gen = CurriculumSmilesGenerator(val_data, vocab)\n",
    "\n",
    "    # 4. SETUP MODELLO\n",
    "    logger.info(\"ğŸ—ï¸ Costruzione Modello...\")\n",
    "    model = build_improved_model(len(vocab))\n",
    "    model.summary()\n",
    "\n",
    "    # 5. AVVIO TRAINING\n",
    "    logger.info(\"ğŸ”¥ Inizio Training Loop...\")\n",
    "    \n",
    "    callbacks = [\n",
    "        CustomTensorBoard(log_dir=f\"logs/run_loaded_{int(time.time())}\"),\n",
    "        EnhancedTrainingMonitor(val_gen),\n",
    "        ModelCheckpoint(\"best_hybrid_model.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "    ]\n",
    "    try:\n",
    "        model.fit(\n",
    "            train_gen.get_dataset(),\n",
    "            epochs=config.EPOCHS,\n",
    "            steps_per_epoch=config.STEPS_PER_EPOCH,\n",
    "            validation_data=val_gen.get_dataset(),\n",
    "            validation_steps=max(1, len(val_data)//config.BATCH_SIZE),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        model.save(\"final_hybrid_model.keras\")\n",
    "        logger.info(\"ğŸ† Training Completato.\")\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"âš ï¸ Training interrotto dall'utente.\")\n",
    "        model.save(\"interrupted_hybrid.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee50283a-236d-4ba6-a663-ac1e30e761c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': None}.\n\nException encountered: Could not locate class 'ImprovedTransformerBlock'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'ImprovedTransformerBlock', 'config': {'name': 'improved_transformer_block', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 139862845535040}, 'embed_dim': 512, 'num_heads': 6, 'ffn_dim': 2048, 'rate': 0.1}, 'registered_name': 'ImprovedTransformerBlock', 'build_config': {'input_shape': [None, 140, 512]}, 'name': 'improved_transformer_block', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 140, 512], 'dtype': 'float32', 'keras_history': ['dropout', 0, 0]}}], 'kwargs': {'training': False, 'mask': None}}]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/models/model.py:587\u001b[0m, in \u001b[0;36mModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[0;32m--> 587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/models/functional.py:557\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m functional_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 557\u001b[0m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/models/functional.py:524\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Operation):\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:694\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, types\u001b[38;5;241m.\u001b[39mFunctionType):\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:810\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    805\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not deserialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    806\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mits parent module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be imported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    807\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    808\u001b[0m             )\n\u001b[0;32m--> 810\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure custom classes are decorated with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not locate class 'ImprovedTransformerBlock'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'ImprovedTransformerBlock', 'config': {'name': 'improved_transformer_block', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 139862845535040}, 'embed_dim': 512, 'num_heads': 6, 'ffn_dim': 2048, 'rate': 0.1}, 'registered_name': 'ImprovedTransformerBlock', 'build_config': {'input_shape': [None, 140, 512]}, 'name': 'improved_transformer_block', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 140, 512], 'dtype': 'float32', 'keras_history': ['dropout', 0, 0]}}], 'kwargs': {'training': False, 'mask': None}}]}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cfg\n\u001b[1;32m     34\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/grad/Desktop/pietro/denovo/2/final_hybrid_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDynamicPositionalEncoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDynamicPositionalEncoding\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# avoids needing custom loss/optimizer objects\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Print summary and counts\u001b[39;00m\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/saving/saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:367\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m     )\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:444\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(_CONFIG_FILENAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    442\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 444\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    449\u001b[0m extract_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:433\u001b[0m, in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 433\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:720\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_config(inner_config)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    723\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instances (layers, models, etc.) returned by\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `get_config()` are explicitly deserialized in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms `from_config()` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m     )\n\u001b[1;32m    728\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m build_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mbuilt:\n",
      "\u001b[0;31mTypeError\u001b[0m: <class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': None}.\n\nException encountered: Could not locate class 'ImprovedTransformerBlock'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'ImprovedTransformerBlock', 'config': {'name': 'improved_transformer_block', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 139862845535040}, 'embed_dim': 512, 'num_heads': 6, 'ffn_dim': 2048, 'rate': 0.1}, 'registered_name': 'ImprovedTransformerBlock', 'build_config': {'input_shape': [None, 140, 512]}, 'name': 'improved_transformer_block', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 140, 512], 'dtype': 'float32', 'keras_history': ['dropout', 0, 0]}}], 'kwargs': {'training': False, 'mask': None}}]}"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7dd37-65da-455a-b149-3ee8eb723edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
