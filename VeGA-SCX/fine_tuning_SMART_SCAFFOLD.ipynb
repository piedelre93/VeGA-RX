{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86c504c-b5dc-4b4d-a6d5-5e4e0cc810a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/grad/Desktop/pietro/denovo/finetuning_results_gba\n"
     ]
    }
   ],
   "source": [
    "cd /home/grad/Desktop/pietro/denovo/finetuning_results_gba/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88de0cf8-dd45-42c0-8612-a625a79c03a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 20:06:09.949559: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-30 20:06:09.963258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769799969.980070  295009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769799969.985103  295009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769799969.997926  295009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769799969.997947  295009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769799969.997948  295009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769799969.997950  295009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-30 20:06:10.002212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles, FilterCatalog\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Tuple, Optional\n",
    "import re\n",
    "from threading import Lock\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Importiamo le classi custom per permettere a Keras di caricare il modello\n",
    "from tensorflow.keras.layers import Layer, Embedding, Input, LayerNormalization, MultiHeadAttention, Dropout, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "# ====================================================================\n",
    "# [ 1. CONFIGURAZIONE FINE-TUNING ]\n",
    "# ====================================================================\n",
    "\n",
    "class config:\n",
    "    # --- Percorsi File Esistenti (Dal primo training) ---\n",
    "    PRETRAINED_MODEL = \"/home/grad/Desktop/pietro/denovo/2/final_hybrid_model.keras\"\n",
    "    VOCAB_PATH = \"/home/grad/Desktop/pietro/denovo/2/vocab.json\"\n",
    "    SMARTS_RX_FILE = \"/home/grad/Desktop/pietro/denovo/2/SMART_RX/smartsrx-main/smartsrx.json\"\n",
    "\n",
    "    # --- Percorsi Nuovi Dati ---\n",
    "    SMILES_FILE = \"/home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/s4_loro/gen_mio/eval_out_gba/train_gba.smi\" \n",
    "    SAVE_DIR = \"finetuning_results_gba\"\n",
    "    PRINT_EVERY = 100\n",
    "    # --- Iperparametri Fine-Tuning ---\n",
    "    # Usiamo un Learning Rate pi√π basso (es. 1e-5 o 5e-6)\n",
    "    LR = 1e-5 \n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 50\n",
    "    MAX_LENGTH = 140 # Deve essere uguale a quella del modello originale\n",
    "    AUGMENT_PROB = 0.1\n",
    "    VALID_RATIO = 0.1\n",
    "    L2_REG = 1e-4\n",
    "    CURRICULUM_START_COMPLEXITY = 1\n",
    "    STEPS_PER_EPOCH = 100\n",
    "        # Iperparametri Modello\n",
    "    EMBED_DIM = 512\n",
    "    TRANSFORMER_LAYERS = 6\n",
    "    TRANSFORMER_HEADS = 6\n",
    "    FF_DIM = 2048\n",
    "    DROPOUT_RATE = 0.10\n",
    "    L2_REG = 1e-4\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 10\n",
    "    EPOCHS = 3\n",
    "    MAX_LENGTH = 140         # Aumentato leggermente per ospitare 3 parti\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    VALID_RATIO = 0.1\n",
    "    \n",
    "    # Curriculum & Augmentation\n",
    "    CURRICULUM_START_COMPLEXITY = 10\n",
    "    CURRICULUM_COMPLEXITY_STEP = 5\n",
    "    LOSS_STABILITY_THRESHOLD = 0.01\n",
    "    WARMUP_EPOCHS = 5\n",
    "    AUGMENT_PROB = 0.1\n",
    "    \n",
    "    # Generazione\n",
    "    TEMPERATURE = 1.0\n",
    "    GEN_NUM = 10\n",
    "    PRINT_EVERY = 100\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "os.makedirs(config.SAVE_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ====================================================================\n",
    "# [ 2. DEFINIZIONI CUSTOM (Necessarie per caricare il modello) ]\n",
    "# ====================================================================\n",
    "\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention, Input\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "# Loss\n",
    "def smoothed_loss(y_true, y_pred):\n",
    "    y_true_int = tf.cast(y_true, tf.int32)\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_int, logits=y_pred)\n",
    "    return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "# Layers\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        max_len = config.MAX_LENGTH\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"embed_dim\": self.embed_dim}\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim, \n",
    "            kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\", kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)),\n",
    "            Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG))\n",
    "        ])\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.d1 = Dropout(rate)\n",
    "        self.d2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        out1 = self.ln1(inputs + self.d1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.ln2(out1 + self.d2(ffn_output, training=training))\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"embed_dim\": self.embed_dim, \"num_heads\": self.num_heads, \"ffn_dim\": self.ffn_dim, \"rate\": self.rate}\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": float(self.embed_dim), \"warmup_steps\": float(self.warmup_steps)}\n",
    "\n",
    "# ====================================================================\n",
    "# [ 3. UTILITY CHIMICHE (Copiate dall'originale) ]\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# [ 2. GESTIONE SMARTS-RX & SCAFFOLD ]\n",
    "# ====================================================================\n",
    "\n",
    "# Variabili Globali Catalogo\n",
    "SMARTS_CATALOG = None\n",
    "\n",
    "def initialize_smarts_catalog():\n",
    "    \"\"\"Carica il catalogo SMARTS-RX.\"\"\"\n",
    "    global SMARTS_CATALOG\n",
    "    if SMARTS_CATALOG is not None: return\n",
    "\n",
    "    try:\n",
    "        with open(config.SMARTS_RX_FILE, \"rt\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Errore caricamento SMARTS JSON: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    catalog = FilterCatalog.FilterCatalog()\n",
    "    count = 0\n",
    "    for entry in data.get(\"data\", []):\n",
    "        name = entry.get(\"specific_type\")\n",
    "        smarts = entry.get(\"smarts\")\n",
    "        if name and smarts:\n",
    "            pattern = Chem.MolFromSmarts(smarts)\n",
    "            if pattern:\n",
    "                catalog.AddEntry(FilterCatalog.FilterCatalogEntry(name, FilterCatalog.SmartsMatcher(pattern)))\n",
    "                count += 1\n",
    "    SMARTS_CATALOG = catalog\n",
    "    logger.info(f\"Catalogo SMARTS-RX caricato: {count} regole.\")\n",
    "\n",
    "def get_smarts_fingerprint(mol: Chem.Mol) -> List[str]:\n",
    "    \"\"\"Restituisce lista nomi gruppi funzionali presenti.\"\"\"\n",
    "    if not mol or SMARTS_CATALOG is None: return []\n",
    "    matches = SMARTS_CATALOG.GetMatches(mol)\n",
    "    # Set per unicit√†, sorted per ordine deterministico\n",
    "    return sorted(list(set([m.GetDescription() for m in matches])))\n",
    "\n",
    "def get_murcko_scaffold_tokens(mol: Chem.Mol) -> List[str]:\n",
    "    \"\"\"Estrae scaffold e lo tokenizza.\"\"\"\n",
    "    if not mol: return []\n",
    "    try:\n",
    "        scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "        smi = Chem.MolToSmiles(scaffold, canonical=True, isomericSmiles=False)\n",
    "        if not smi: return []\n",
    "        return robust_tokenize(smi)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# ====================================================================\n",
    "# [ 3. HELPER CHIMICI BASE ]\n",
    "# ====================================================================\n",
    "\n",
    "def robust_tokenize(smiles: str) -> list:\n",
    "    pattern = (\n",
    "        r\"(\\[[^\\[\\]]{1,6}\\]|\"                 # atomi in parentesi quadre\n",
    "        r\"Br|Cl|Si|Na|Mg|Mn|Ca|Fe|Zn|Se|Li|K|Al|B|\"  # elementi multi-char\n",
    "        r\"R[0-9]|r[0-9]|a[0-9]|\"             # ring labels\n",
    "        r\"[A-Za-z0-9@+\\-\\\\\\/\\(\\)=#\\$\\.\\%,])\"  # singoli caratteri, incluso '%'\n",
    "    )\n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    stack = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('['): stack.append(t)\n",
    "        if t.endswith(']'):\n",
    "            if not stack: return []\n",
    "            stack.pop()\n",
    "    return tokens\n",
    "\n",
    "def validate_and_fix_smiles(smiles: str) -> str:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is None: return None\n",
    "        try: Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        except: pass\n",
    "        return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "    except: return None\n",
    "\n",
    "def randomize_smiles(smiles: str, num_versions: int = 1) -> List[str]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return []\n",
    "    res = []\n",
    "    try:\n",
    "        s = Chem.MolToSmiles(mol, doRandom=True, canonical=False)\n",
    "        if s: res.append(s)\n",
    "    except: pass\n",
    "    return res\n",
    "def compute_complexity_from_tokens(tokens: List[str]) -> int:\n",
    "    smiles = ''.join(tokens)\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol: return 999\n",
    "        return Chem.GetSSSR(mol) + smiles.count('(')\n",
    "    except: return 999\n",
    "# ====================================================================\n",
    "# [ 4. PREPARAZIONE DATI FINE-TUNING ]\n",
    "# ====================================================================\n",
    "\n",
    "def process_dataset(smiles_list, vocab):\n",
    "    initialize_smarts_catalog()\n",
    "    char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "    processed = []\n",
    "    \n",
    "    logger.info(f\"Processando {len(smiles_list)} SMILES per fine-tuning...\")\n",
    "    for s in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if not mol: continue\n",
    "        \n",
    "        target_s = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "        tokens_target = robust_tokenize(target_s)\n",
    "        \n",
    "        # Scaffold\n",
    "        scaf_mol = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "        scaf_smi = Chem.MolToSmiles(scaf_mol, canonical=True)\n",
    "        tokens_scaf = robust_tokenize(scaf_smi)\n",
    "        \n",
    "        # SMARTS\n",
    "        tokens_smarts = get_smarts_fingerprint(mol)\n",
    "        \n",
    "        # Controllo se i token esistono nel vecchio vocabolario\n",
    "        # Se un nuovo SMILES ha un atomo mai visto prima, verr√† marcato come <UNK>\n",
    "        processed.append((tokens_smarts, tokens_scaf, tokens_target))\n",
    "    logger.info(f\"DONE. Valid: {len(processed)}.\")\n",
    "\n",
    "    return processed\n",
    "\n",
    "# ====================================================================\n",
    "# [ 5. CLASSI THREAD-SAFE ]\n",
    "# ====================================================================\n",
    "\n",
    "class ThreadSafeIterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = Lock()\n",
    "    def __iter__(self): return self\n",
    "    def __next__(self):\n",
    "        with self.lock: return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    def wrapper(*args, **kwargs): return ThreadSafeIterator(func(*args, **kwargs))\n",
    "    return wrapper\n",
    "\n",
    "# ====================================================================\n",
    "# [ 6. GENERATORE CURRICULUM (3 PARTI) ]\n",
    "# ====================================================================\n",
    "\n",
    "class CurriculumSmilesGenerator:\n",
    "    def __init__(self, processed_data, vocab: List[str]):\n",
    "        self.char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        self.original_data = []\n",
    "        \n",
    "        # data = (smarts, scaffold, target)\n",
    "        for smarts, scaffold, target in processed_data:\n",
    "            comp = compute_complexity_from_tokens(target)\n",
    "            self.original_data.append(((smarts, scaffold, target), comp))\n",
    "            \n",
    "        valid_comps = [c for _, c in self.original_data if c != 999]\n",
    "        self.max_complexity = max(valid_comps) if valid_comps else 0\n",
    "        self.current_complexity = config.CURRICULUM_START_COMPLEXITY\n",
    "        self.available_data = self._filter_data()\n",
    "        # Per novelty check\n",
    "        self.train_smiles = {''.join(t) for (_, _, t), _ in self.original_data}\n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def _filter_data(self):\n",
    "        filtered = [dp for dp, c in self.original_data if c <= self.current_complexity]\n",
    "        return filtered if filtered else [dp for dp, _ in self.original_data]\n",
    "    \n",
    "    def update_complexity(self, epoch: int, loss_diff: float = None):\n",
    "        with self.lock:\n",
    "            if loss_diff is not None and loss_diff < config.LOSS_STABILITY_THRESHOLD:\n",
    "                self.current_complexity = min(self.current_complexity + config.CURRICULUM_COMPLEXITY_STEP, self.max_complexity)\n",
    "            else:\n",
    "                if epoch <= config.WARMUP_EPOCHS:\n",
    "                    incr = int((self.max_complexity - config.CURRICULUM_START_COMPLEXITY) * (epoch / config.WARMUP_EPOCHS))\n",
    "                    self.current_complexity = config.CURRICULUM_START_COMPLEXITY + incr\n",
    "                else:\n",
    "                    self.current_complexity = self.max_complexity\n",
    "            \n",
    "            self.available_data = self._filter_data()\n",
    "            if not self.available_data:\n",
    "                self.available_data = [dp for dp, _ in self.original_data]\n",
    "                logger.warning(\"Reset available_data (fallback).\")\n",
    "    \n",
    "    @threadsafe_generator\n",
    "    def __call__(self):\n",
    "        PAD_IDX = self.char2idx['<PAD>']\n",
    "        UNK_IDX = self.char2idx.get('<UNK>', PAD_IDX)\n",
    "\n",
    "        while True:\n",
    "            inputs = np.full((config.BATCH_SIZE, config.MAX_LENGTH), PAD_IDX, dtype=np.int32)\n",
    "            targets = np.full_like(inputs, PAD_IDX)\n",
    "            \n",
    "            for i in range(config.BATCH_SIZE):\n",
    "                with self.lock:\n",
    "                    try:\n",
    "                        data_pair = random.choice(self.available_data)\n",
    "                    except:\n",
    "                        self.available_data = [dp for dp, _ in self.original_data]\n",
    "                        data_pair = random.choice(self.available_data)\n",
    "                \n",
    "                # Unpacking 3 parti\n",
    "                t_smarts, t_scaf, t_targ = data_pair\n",
    "\n",
    "                # Augmentation solo sul target\n",
    "                curr_target = t_targ\n",
    "                if random.random() < config.AUGMENT_PROB:\n",
    "                    try:\n",
    "                        aug = randomize_smiles(''.join(t_targ), 1)\n",
    "                        if aug:\n",
    "                            tok = robust_tokenize(aug[0])\n",
    "                            if tok: curr_target = tok\n",
    "                    except: pass\n",
    "\n",
    "                # STRUTTURA: [START] SMARTS [SEP] SCAFFOLD [SEP] TARGET [END]\n",
    "                seq = (['<START>'] + \n",
    "                       t_smarts + ['<SEP>'] + \n",
    "                       t_scaf + ['<SEP>'] + \n",
    "                       curr_target + ['<END>'])\n",
    "                \n",
    "                padded_seq = (seq + ['<PAD>'] * config.MAX_LENGTH)[:config.MAX_LENGTH]\n",
    "                indices = [self.char2idx.get(t, UNK_IDX) for t in padded_seq]\n",
    "                \n",
    "                inputs[i] = indices\n",
    "                targets[i, :-1] = inputs[i][1:]\n",
    "                targets[i, -1] = PAD_IDX\n",
    "                \n",
    "            yield inputs, targets\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            self.__call__,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32)\n",
    "            )\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "# ====================================================================\n",
    "# [ 8. MONITORAGGIO (Parsing Complesso) ]\n",
    "# ====================================================================\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "class CustomTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}; lr = self.model.optimizer.learning_rate\n",
    "        logs['lr'] = lr(epoch).numpy() if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule) else lr.numpy()\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class EnhancedTrainingMonitor(Callback):\n",
    "    def __init__(self, val_gen: CurriculumSmilesGenerator):\n",
    "        super().__init__()\n",
    "        self.val_gen = val_gen\n",
    "        self.best_val_loss = np.inf; self.prev_val_loss = None\n",
    "\n",
    "    def generate_sample(self, num: int):\n",
    "        generated, valid = [], []\n",
    "        PAD, START, END, SEP = [self.val_gen.char2idx[k] for k in ['<PAD>','<START>','<END>','<SEP>']]\n",
    "        input_seq = np.full((1, config.MAX_LENGTH), PAD, dtype=np.int32)\n",
    "        \n",
    "        for _ in range(num):\n",
    "            input_seq.fill(PAD); input_seq[0, 0] = START\n",
    "            for t in range(1, config.MAX_LENGTH):\n",
    "                logits = self.model(input_seq, training=False)[0, t-1]\n",
    "                probs = tf.nn.softmax(logits / config.TEMPERATURE).numpy()\n",
    "                if np.sum(probs) < 1e-6: break\n",
    "                sampled = np.random.choice(len(probs), p=probs)\n",
    "                input_seq[0, t] = sampled\n",
    "                if sampled == END: break\n",
    "            \n",
    "            indices = input_seq[0].tolist()\n",
    "            raw_tokens = [self.val_gen.idx2char[i] for i in indices if i not in {PAD, START}]\n",
    "            \n",
    "            # Parsing Logic: START [SMARTS] SEP [SCAFFOLD] SEP [TARGET] END\n",
    "            # Dobbiamo trovare l'ultimo SEP per ottenere il target\n",
    "            smi_str = \"\"\n",
    "            try:\n",
    "                # Conta quanti SEP ci sono\n",
    "                sep_indices = [i for i, x in enumerate(raw_tokens) if x == '<SEP>']\n",
    "                if len(sep_indices) >= 2:\n",
    "                    # Prendi tutto dopo il SECONDO SEP\n",
    "                    target_tokens = raw_tokens[sep_indices[1]+1:]\n",
    "                    smi_str = \"\".join(target_tokens).split('<END>')[0]\n",
    "                elif len(sep_indices) == 1:\n",
    "                    # Fallback: forse ha saltato uno step\n",
    "                    target_tokens = raw_tokens[sep_indices[0]+1:]\n",
    "                    smi_str = \"\".join(target_tokens).split('<END>')[0]\n",
    "                else:\n",
    "                    smi_str = \"\".join(raw_tokens).split('<END>')[0]\n",
    "            except: pass\n",
    "            \n",
    "            final = validate_and_fix_smiles(smi_str)\n",
    "            if final: valid.append(final)\n",
    "            generated.append(final or smi_str)\n",
    "            \n",
    "        return generated, valid\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if self.prev_val_loss and 'val_loss' in logs:\n",
    "            diff = (self.prev_val_loss - logs['val_loss']) / self.prev_val_loss\n",
    "            self.val_gen.update_complexity(epoch, diff)\n",
    "        if logs.get('val_loss', np.inf) < self.best_val_loss: self.best_val_loss = logs['val_loss']\n",
    "        self.prev_val_loss = logs.get('val_loss')\n",
    "        \n",
    "        if (epoch + 1) % config.PRINT_EVERY == 0:\n",
    "            gen, val = self.generate_sample(config.GEN_NUM)\n",
    "            validity = len(val) / config.GEN_NUM\n",
    "            novel = len([s for s in val if s not in self.val_gen.train_smiles])\n",
    "            logger.info(f\"\\nEPOCH {epoch+1}: Validity {validity:.1%} | Novelty {novel}/{len(val)}\")\n",
    "            if val: logger.info(f\"Sample: {val[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ddb9502-e513-4d12-885f-488126ca42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 20:06:12,445 [INFO] Caricamento modello pre-trainato...\n",
      "I0000 00:00:1769799972.546087  295009 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9890 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "2026-01-30 20:06:15,350 [INFO] Catalogo SMARTS-RX caricato: 406 regole.\n",
      "2026-01-30 20:06:15,351 [INFO] Processando 104 SMILES per fine-tuning...\n",
      "2026-01-30 20:06:17,300 [INFO] DONE. Valid: 104.\n",
      "2026-01-30 20:06:17,301 [INFO] ‚öôÔ∏è Inizializzazione Generatori...\n",
      "2026-01-30 20:06:17,319 [INFO] üî• Inizio Training Loop...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769799987.350648  295253 service.cc:152] XLA service 0x7f6ebc00f780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1769799987.350677  295253 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A2000 12GB, Compute Capability 8.6\n",
      "2026-01-30 20:06:28.229265: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1769799990.018577  295253 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2026-01-30 20:06:33.076662: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:33.655058: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 988 bytes spill stores, 884 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:33.803468: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:33.861361: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 432 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:34.344917: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 972 bytes spill stores, 872 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:34.869186: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:35.031564: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 112 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:35.393457: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 440 bytes spill stores, 344 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:36.043195: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:36.160494: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 296 bytes spill stores, 296 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:36.337562: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 540 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:36.522503: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 436 bytes spill stores, 344 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:37.084586: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:37.526509: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 348 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:37.613556: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:37.911622: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178', 228 bytes spill stores, 180 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:37.965400: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 828 bytes spill stores, 1616 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:37.987720: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 900 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:38.577605: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 572 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:38.586491: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:39.514756: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:40.548036: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:40.710940: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:41.463496: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 900 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:41.539520: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59_0', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:41.596786: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178', 676 bytes spill stores, 560 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:42.122360: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 1188 bytes spill stores, 900 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:42.159275: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 156 bytes spill stores, 156 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:42.778361: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:42.847089: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:43.199736: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 224 bytes spill stores, 224 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:43.303122: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 812 bytes spill stores, 1632 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:43.321208: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 1784 bytes spill stores, 1752 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:43.332160: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178', 96 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:43.653117: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59_0', 1288 bytes spill stores, 1288 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:43.749766: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:43.880960: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:44.484700: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 432 bytes spill stores, 432 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:44.638554: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:44.845892: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178', 104 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:45.301280: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 400 bytes spill stores, 400 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:45.384275: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 4048 bytes spill stores, 4032 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:45.391949: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:45.728952: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:45.773574: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 660 bytes spill stores, 660 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:46.213720: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:46.641282: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:46.829031: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 420 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:46.907438: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:47.504599: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 656 bytes spill stores, 656 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:47.650239: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 888 bytes spill stores, 888 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:47.792147: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 300 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:48.438145: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:48.490230: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 540 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:48.934833: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 448 bytes spill stores, 448 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:48.982813: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 876 bytes spill stores, 876 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:49.903557: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:50.004628: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 456 bytes spill stores, 456 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:50.013583: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:50.332829: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 4600 bytes spill stores, 4520 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:50.456862: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:50.751832: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:50.949473: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 1764 bytes spill stores, 1788 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:51.285856: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:51.372423: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 1764 bytes spill stores, 1788 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:51.511698: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:51.752634: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 444 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:51.859084: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 428 bytes spill stores, 428 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:51.946191: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:52.160787: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 888 bytes spill stores, 888 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:52.471029: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 448 bytes spill stores, 448 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:52.536317: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:53.171909: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 456 bytes spill stores, 456 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:53.268747: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:53.483965: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:54.104765: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:54.319207: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:54.753773: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 656 bytes spill stores, 656 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:54.759293: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:54.797325: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10948', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:54.881941: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:55.262962: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10845', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:55.635432: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 480 bytes spill stores, 480 bytes spill loads\n",
      "\n",
      "2026-01-30 20:06:55.657173: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 400 bytes spill stores, 400 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/1000\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m13:24:30\u001b[0m 48s/step - loss: 0.5414"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1769800025.937579  295253 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 95ms/step - loss: 0.3397 - val_loss: 0.3709 - lr: 1.0000e-05\n",
      "Epoch 2/3\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 91ms/step - loss: 0.1835 - val_loss: 0.4548 - lr: 1.0000e-05\n",
      "Epoch 3/3\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 91ms/step - loss: 0.1698 - val_loss: 0.5098 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 20:11:44,941 [INFO] üèÜ Training Completato.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Caricamento Vocabolario Originale\n",
    "    if not os.path.exists(config.VOCAB_PATH):\n",
    "        logger.error(\"Vocabolario originale non trovato! Necessario per FT.\"); sys.exit(1)\n",
    "    with open(config.VOCAB_PATH, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    \n",
    "    # 2. Caricamento Modello con Oggetti Custom\n",
    "    logger.info(\"Caricamento modello pre-trainato...\")\n",
    "    model = load_model(config.PRETRAINED_MODEL, custom_objects = {\n",
    "    \"DynamicPositionalEncoding\": DynamicPositionalEncoding,\n",
    "    \"ImprovedTransformerBlock\": ImprovedTransformerBlock,\n",
    "    \"CustomSchedule\": CustomSchedule,\n",
    "    \"smoothed_loss\": smoothed_loss,\n",
    "})\n",
    "    \n",
    "    # Riduciamo il Learning Rate per il fine-tuning\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config.LR),\n",
    "        loss=smoothed_loss\n",
    "    )\n",
    "\n",
    "    # 3. Caricamento Nuovi Dati\n",
    "    with open(config.SMILES_FILE, \"r\") as f:\n",
    "        new_smiles = [l.strip() for l in f if l.strip()]\n",
    "    \n",
    "    processed_data = process_dataset(new_smiles, vocab)\n",
    "    train_data, val_data = train_test_split(processed_data, test_size=config.VALID_RATIO)\n",
    "    # 3. SETUP GENERATORI\n",
    "    logger.info(\"‚öôÔ∏è Inizializzazione Generatori...\")\n",
    "    train_gen = CurriculumSmilesGenerator(train_data, vocab)\n",
    "    val_gen = CurriculumSmilesGenerator(val_data, vocab)\n",
    "\n",
    "\n",
    "    # 5. AVVIO TRAINING\n",
    "    logger.info(\"üî• Inizio Training Loop...\")\n",
    "    \n",
    "    callbacks = [\n",
    "        CustomTensorBoard(log_dir=f\"logs/run_loaded_{int(time.time())}\"),\n",
    "        EnhancedTrainingMonitor(val_gen),\n",
    "        ModelCheckpoint(\"best_hybrid_model.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "    ]\n",
    "    try:\n",
    "        model.fit(\n",
    "            train_gen.get_dataset(),\n",
    "            epochs=config.EPOCHS,\n",
    "            steps_per_epoch=config.STEPS_PER_EPOCH,\n",
    "            validation_data=val_gen.get_dataset(),\n",
    "            validation_steps=max(1, len(val_data)//config.BATCH_SIZE),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        model.save(\"final_hybrid_model.keras\")\n",
    "        logger.info(\"üèÜ Training Completato.\")\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"‚ö†Ô∏è Training interrotto dall'utente.\")\n",
    "        model.save(\"interrupted_hybrid.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee88fb-5563-421f-a96d-cc5b57561cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
