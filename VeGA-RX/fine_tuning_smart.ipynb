{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4a0bba-5ecd-4ad5-adc9-bcc0564a7f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/grad/Desktop/pietro/denovo/2/smartrx/finetuning/mapk1\n"
     ]
    }
   ],
   "source": [
    "cd '/home/grad/Desktop/pietro/denovo/2/smartrx/finetuning/mapk1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c7a91b2-38cb-4f3e-b892-b55f568e3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 09:46:08.335049: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-30 09:46:08.350112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769762768.367702 2369230 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769762768.373052 2369230 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769762768.386330 2369230 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769762768.386349 2369230 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769762768.386350 2369230 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769762768.386352 2369230 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-30 09:46:08.390980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING: SMARTS-RX -> SMILES (NO SCAFFOLD) - FINE TUNING\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from threading import Lock\n",
    "from typing import List, Tuple\n",
    "\n",
    "# RDKit Imports\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogEntry, SmartsMatcher\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow Imports\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention, Input\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# ====================================================================\n",
    "# [ 0. GPU SETUP ]\n",
    "# ====================================================================\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# ====================================================================\n",
    "# [ 1. CONFIGURATION ]\n",
    "# ====================================================================\n",
    "\n",
    "class Config:\n",
    "    # --- PATHS ---\n",
    "    SMILES_FILE = \"/home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/s4_loro/gen_mio/eval_out_mapk1/train.smi\"\n",
    "    SMARTS_RX_FILE = \"/home/grad/Desktop/pietro/denovo/2/SMART_RX/smartsrx-main/smartsrx.json\"\n",
    "    VOCAB_PATH = \"/home/grad/Desktop/pietro/denovo/2/smartrx/vocab.json\"\n",
    "    PRETRAINED_MODEL = \"/home/grad/Desktop/pietro/denovo/2/smartrx/best_hybrid_model.keras\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    EMBED_DIM = 512\n",
    "    LR = 1e-5  # Low learning rate for fine-tuning\n",
    "    TRANSFORMER_LAYERS = 6\n",
    "    TRANSFORMER_HEADS = 6\n",
    "    FF_DIM = 2048\n",
    "    DROPOUT_RATE = 0.10\n",
    "    L2_REG = 1e-4\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 5\n",
    "    MAX_LENGTH = 100\n",
    "    VALID_RATIO = 0.1\n",
    "    \n",
    "    # Curriculum & Augmentation\n",
    "    CURRICULUM_START_COMPLEXITY = 10\n",
    "    CURRICULUM_COMPLEXITY_STEP = 1\n",
    "    LOSS_STABILITY_THRESHOLD = 0.01\n",
    "    WARMUP_EPOCHS = 5\n",
    "    AUGMENT_PROB = 0.10\n",
    "    \n",
    "    # Generation Monitor\n",
    "    TEMPERATURE = 1.0\n",
    "    GEN_NUM = 10\n",
    "    PRINT_EVERY = 10  \n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Logger Config\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "# ====================================================================\n",
    "# [ 2. CUSTOM LAYERS & LOSS ]\n",
    "# ====================================================================\n",
    "\n",
    "def smoothed_loss(y_true, y_pred):\n",
    "    y_true_int = tf.cast(y_true, tf.int32)\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_int, logits=y_pred)\n",
    "    return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        max_len = config.MAX_LENGTH\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"embed_dim\": self.embed_dim}\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        # Sub-layers defined in __init__\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim, \n",
    "            kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\", kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)),\n",
    "            Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG))\n",
    "        ])\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.d1 = Dropout(rate)\n",
    "        self.d2 = Dropout(rate)\n",
    "\n",
    "    # --- FIX: ADDED BUILD METHOD TO SUPPRESS WARNINGS ---\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        # Keras will now correctly track the state of this layer\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        out1 = self.ln1(inputs + self.d1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.ln2(out1 + self.d2(ffn_output, training=training))\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            **super().get_config(), \n",
    "            \"embed_dim\": self.embed_dim, \n",
    "            \"num_heads\": self.num_heads, \n",
    "            \"ffn_dim\": self.ffn_dim, \n",
    "            \"rate\": self.rate\n",
    "        }\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": float(self.embed_dim), \"warmup_steps\": float(self.warmup_steps)}\n",
    "\n",
    "# ====================================================================\n",
    "# [ 3. SMARTS-RX & CHEMICAL HELPERS ]\n",
    "# ====================================================================\n",
    "\n",
    "SMARTS_CATALOG = None\n",
    "\n",
    "def initialize_smarts_catalog():\n",
    "    global SMARTS_CATALOG\n",
    "    if SMARTS_CATALOG is not None: return\n",
    "\n",
    "    try:\n",
    "        with open(config.SMARTS_RX_FILE, \"rt\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading SMARTS JSON: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    catalog = FilterCatalog()\n",
    "    count = 0\n",
    "    for entry in data.get(\"data\", []):\n",
    "        name = entry.get(\"specific_type\")\n",
    "        smarts = entry.get(\"smarts\")\n",
    "        if name and smarts:\n",
    "            pattern = Chem.MolFromSmarts(smarts)\n",
    "            if pattern:\n",
    "                catalog.AddEntry(FilterCatalogEntry(name, SmartsMatcher(pattern)))\n",
    "                count += 1\n",
    "    SMARTS_CATALOG = catalog\n",
    "    logger.info(f\"‚úì SMARTS-RX catalog loaded: {count} rules.\")\n",
    "\n",
    "def get_smarts_fingerprint(mol: Chem.Mol) -> List[str]:\n",
    "    if not mol or SMARTS_CATALOG is None: return []\n",
    "    matches = SMARTS_CATALOG.GetMatches(mol)\n",
    "    return sorted(list(set([m.GetDescription() for m in matches])))\n",
    "\n",
    "def robust_tokenize(smiles: str) -> list:\n",
    "    pattern = (\n",
    "        r\"(\\[[^\\[\\]]{1,6}\\]|\"                 \n",
    "        r\"Br|Cl|Si|Na|Mg|Mn|Ca|Fe|Zn|Se|Li|K|Al|B|\" \n",
    "        r\"R[0-9]|r[0-9]|a[0-9]|\"             \n",
    "        r\"[A-Za-z0-9@+\\-\\\\\\/\\(\\)=#\\$\\.\\%,])\"  \n",
    "    )\n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    if tokens.count('[') != tokens.count(']'): return []\n",
    "    return tokens\n",
    "\n",
    "def validate_and_fix_smiles(smiles: str) -> str:\n",
    "    if not smiles: return None\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is None: return None\n",
    "        try: Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        except: pass\n",
    "        return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "    except: return None\n",
    "\n",
    "def randomize_smiles(smiles: str, num_versions: int = 1) -> List[str]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return []\n",
    "    res = []\n",
    "    try:\n",
    "        s = Chem.MolToSmiles(mol, doRandom=True, canonical=False)\n",
    "        if s: res.append(s)\n",
    "    except: pass\n",
    "    return res\n",
    "\n",
    "def compute_complexity_from_tokens(tokens: List[str]) -> int:\n",
    "    smiles = ''.join(tokens)\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol: return 999\n",
    "        return Chem.GetSSSR(mol) + smiles.count('(')\n",
    "    except: return 999\n",
    "\n",
    "# ====================================================================\n",
    "# [ 4. DATA PROCESSING ]\n",
    "# ====================================================================\n",
    "\n",
    "def process_dataset(smiles_list, vocab):\n",
    "    initialize_smarts_catalog()\n",
    "    \n",
    "    # Prepariamoci a gestire UNK se necessario\n",
    "    vocab_set = set(vocab)\n",
    "    processed = []\n",
    "    \n",
    "    logger.info(f\"Processing {len(smiles_list)} SMILES (Auto-Kekulization ON)...\")\n",
    "    \n",
    "    for idx, s in enumerate(smiles_list):\n",
    "        if idx % 5000 == 0 and idx > 0: logger.info(f\"Processed {idx}...\")\n",
    "            \n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if not mol: continue\n",
    "        \n",
    "        # --- FIX IMPORTANTE: KEKULIZATION ---\n",
    "        # Se 'c' manca nel vocab ma 'C' c'√®, questo risolve il problema\n",
    "        try:\n",
    "            Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        except Exception:\n",
    "            # Se fallisce kekulizzazione, skippiamo\n",
    "            continue\n",
    "            \n",
    "        # Genera SMILES non isomerico (spesso i vocab vecchi non hanno @)\n",
    "        target_s = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "        tokens_target = robust_tokenize(target_s)\n",
    "        \n",
    "        # SMARTS\n",
    "        tokens_smarts = get_smarts_fingerprint(mol)\n",
    "        \n",
    "        # Qui NON scartiamo pi√π tutto se manca un token.\n",
    "        # Lasciamo che il generatore usi <UNK> se proprio serve, \n",
    "        # ma la Kekulizzazione dovrebbe aver risolto il 99% dei 'c' mancanti.\n",
    "        \n",
    "        processed.append((tokens_smarts, tokens_target))\n",
    "        \n",
    "    logger.info(f\"DONE. Valid data: {len(processed)}\")\n",
    "    return processed\n",
    "\n",
    "# ====================================================================\n",
    "# [ 5. CURRICULUM GENERATOR ]\n",
    "# ====================================================================\n",
    "\n",
    "class ThreadSafeIterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = Lock()\n",
    "    def __iter__(self): return self\n",
    "    def __next__(self):\n",
    "        with self.lock: return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    def wrapper(*args, **kwargs): return ThreadSafeIterator(func(*args, **kwargs))\n",
    "    return wrapper\n",
    "\n",
    "class CurriculumSmilesGenerator:\n",
    "    def __init__(self, processed_data: List[Tuple[List[str], List[str]]], vocab: List[str]):\n",
    "        self.char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        \n",
    "        for token in ['<PAD>', '<START>', '<END>', '<SEP>']:\n",
    "            if token not in self.char2idx:\n",
    "                raise ValueError(f\"Essential token '{token}' missing in vocab!\")\n",
    "        \n",
    "        self.original_data = []\n",
    "        for smarts, target in processed_data:\n",
    "            comp = compute_complexity_from_tokens(target)\n",
    "            self.original_data.append(((smarts, target), comp))\n",
    "        \n",
    "        valid_comps = [c for _, c in self.original_data if c != 999]\n",
    "        self.max_complexity = max(valid_comps) if valid_comps else 0\n",
    "        self.current_complexity = config.CURRICULUM_START_COMPLEXITY\n",
    "        self.available_data = self._filter_data()\n",
    "        self.train_smiles = {''.join(target) for (_, target), _ in self.original_data}\n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def _filter_data(self):\n",
    "        filtered = [dp for dp, c in self.original_data if c <= self.current_complexity]\n",
    "        return filtered if filtered else [dp for dp, _ in self.original_data]\n",
    "    \n",
    "    def update_complexity(self, epoch: int, loss_diff: float = None):\n",
    "        with self.lock:\n",
    "            if loss_diff is not None and loss_diff < config.LOSS_STABILITY_THRESHOLD:\n",
    "                self.current_complexity = min(\n",
    "                    self.current_complexity + config.CURRICULUM_COMPLEXITY_STEP, \n",
    "                    self.max_complexity\n",
    "                )\n",
    "            else:\n",
    "                if epoch <= config.WARMUP_EPOCHS:\n",
    "                    incr = int((self.max_complexity - config.CURRICULUM_START_COMPLEXITY) * \n",
    "                              (epoch / config.WARMUP_EPOCHS))\n",
    "                    self.current_complexity = config.CURRICULUM_START_COMPLEXITY + incr\n",
    "                else:\n",
    "                    self.current_complexity = self.max_complexity\n",
    "            \n",
    "            self.available_data = self._filter_data()\n",
    "            if not self.available_data:\n",
    "                self.available_data = [dp for dp, _ in self.original_data]\n",
    "    \n",
    "    @threadsafe_generator\n",
    "    def __call__(self):\n",
    "        PAD_IDX = self.char2idx['<PAD>']\n",
    "        \n",
    "        while True:\n",
    "            inputs = np.full((config.BATCH_SIZE, config.MAX_LENGTH), PAD_IDX, dtype=np.int32)\n",
    "            targets = np.full_like(inputs, PAD_IDX)\n",
    "            \n",
    "            for i in range(config.BATCH_SIZE):\n",
    "                with self.lock:\n",
    "                    if not self.available_data:\n",
    "                        self.available_data = [dp for dp, _ in self.original_data]\n",
    "                    data_pair = random.choice(self.available_data)\n",
    "                \n",
    "                smarts_names, target_tokens = data_pair\n",
    "                curr_target = target_tokens\n",
    "                if random.random() < config.AUGMENT_PROB:\n",
    "                    try:\n",
    "                        aug = randomize_smiles(''.join(target_tokens), 1)\n",
    "                        if aug:\n",
    "                            tok = robust_tokenize(aug[0])\n",
    "                            if tok and all(t in self.char2idx for t in tok):\n",
    "                                curr_target = tok\n",
    "                    except: pass\n",
    "\n",
    "                seq = (['<START>'] + smarts_names + ['<SEP>'] + curr_target + ['<END>'])\n",
    "                padded_seq = (seq + ['<PAD>'] * config.MAX_LENGTH)[:config.MAX_LENGTH]\n",
    "                \n",
    "                try:\n",
    "                    indices = [self.char2idx[t] for t in padded_seq]\n",
    "                except KeyError:\n",
    "                    seq = (['<START>'] + smarts_names + ['<SEP>'] + target_tokens + ['<END>'])\n",
    "                    padded_seq = (seq + ['<PAD>'] * config.MAX_LENGTH)[:config.MAX_LENGTH]\n",
    "                    indices = [self.char2idx[t] for t in padded_seq]\n",
    "\n",
    "                inputs[i] = indices\n",
    "                targets[i, :-1] = indices[1:]\n",
    "                targets[i, -1] = PAD_IDX\n",
    "                \n",
    "            yield inputs, targets\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            self.__call__,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32)\n",
    "            )\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ====================================================================\n",
    "# [ 6. MONITORING ]\n",
    "# ====================================================================\n",
    "\n",
    "class CustomTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        logs['lr'] = lr(epoch).numpy() if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule) else lr.numpy()\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class EnhancedTrainingMonitor(Callback):\n",
    "    def __init__(self, val_gen: CurriculumSmilesGenerator):\n",
    "        super().__init__()\n",
    "        self.val_gen = val_gen\n",
    "        self.best_val_loss = np.inf\n",
    "        self.prev_val_loss = None\n",
    "\n",
    "    def generate_sample(self, num: int):\n",
    "        generated, valid = [], []\n",
    "        PAD = self.val_gen.char2idx['<PAD>']\n",
    "        START = self.val_gen.char2idx['<START>']\n",
    "        END = self.val_gen.char2idx['<END>']\n",
    "        \n",
    "        input_seq = np.full((1, config.MAX_LENGTH), PAD, dtype=np.int32)\n",
    "        \n",
    "        for _ in range(num):\n",
    "            input_seq.fill(PAD)\n",
    "            input_seq[0, 0] = START\n",
    "            \n",
    "            for t in range(1, config.MAX_LENGTH):\n",
    "                logits = self.model(input_seq, training=False)[0, t-1]\n",
    "                logits = logits / config.TEMPERATURE\n",
    "                probs = tf.nn.softmax(logits).numpy()\n",
    "                \n",
    "                sampled = np.random.choice(len(probs), p=probs)\n",
    "                input_seq[0, t] = sampled\n",
    "                if sampled == END: break\n",
    "            \n",
    "            indices = input_seq[0].tolist()\n",
    "            raw_tokens = [self.val_gen.idx2char[i] for i in indices if i not in {PAD, START}]\n",
    "            \n",
    "            try:\n",
    "                if '<SEP>' in raw_tokens:\n",
    "                    target_tokens = raw_tokens[raw_tokens.index('<SEP>') + 1:]\n",
    "                else:\n",
    "                    target_tokens = raw_tokens\n",
    "                \n",
    "                if '<END>' in target_tokens:\n",
    "                    target_tokens = target_tokens[:target_tokens.index('<END>')]\n",
    "                \n",
    "                smi_str = \"\".join(target_tokens)\n",
    "            except Exception:\n",
    "                smi_str = \"\"\n",
    "            \n",
    "            final = validate_and_fix_smiles(smi_str)\n",
    "            if final: valid.append(final)\n",
    "            generated.append(final or smi_str)\n",
    "            \n",
    "        return generated, valid\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if self.prev_val_loss and 'val_loss' in logs:\n",
    "            diff = (self.prev_val_loss - logs['val_loss']) / (self.prev_val_loss + 1e-9)\n",
    "            self.val_gen.update_complexity(epoch, diff)\n",
    "        \n",
    "        if logs.get('val_loss', np.inf) < self.best_val_loss:\n",
    "            self.best_val_loss = logs['val_loss']\n",
    "        \n",
    "        self.prev_val_loss = logs.get('val_loss')\n",
    "        \n",
    "        if (epoch + 1) % config.PRINT_EVERY == 0:\n",
    "            gen, val = self.generate_sample(config.GEN_NUM)\n",
    "            validity = len(val) / config.GEN_NUM\n",
    "            novel = len([s for s in val if s not in self.val_gen.train_smiles])\n",
    "            logger.info(f\"\\n--- EPOCH {epoch+1} RESULTS ---\")\n",
    "            logger.info(f\"Validity: {validity:.1%} | Novelty: {novel}/{len(val)}\")\n",
    "            if val: logger.info(f\"Sample: {val[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "080a7d84-bb75-41dc-a8e5-18a7dab8f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 09:46:10,872 [INFO] Loading pretrained model from: /home/grad/Desktop/pietro/denovo/2/smartrx/best_hybrid_model.keras\n",
      "I0000 00:00:1769762770.951577 2369230 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8729 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2026-01-30 09:46:13,376 [INFO] Model loaded and recompiled.\n",
      "2026-01-30 09:46:13,424 [INFO] ‚úì SMARTS-RX catalog loaded: 406 rules.\n",
      "2026-01-30 09:46:13,425 [INFO] Processing 197 SMILES (Auto-Kekulization ON)...\n",
      "2026-01-30 09:46:15,395 [INFO] DONE. Valid data: 197\n",
      "2026-01-30 09:46:15,397 [INFO] Train size: 177 | Val size: 20\n",
      "2026-01-30 09:46:15,433 [INFO] üî• Starting Fine-Tuning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769762786.247560 2369551 service.cc:152] XLA service 0x712548004170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1769762786.247575 2369551 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A2000 12GB, Compute Capability 8.6\n",
      "2026-01-30 09:46:27.170415: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1769762789.133384 2369551 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2026-01-30 09:46:32.392183: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:33.019997: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 972 bytes spill stores, 872 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:33.198552: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 540 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:33.569468: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 432 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:33.878125: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:34.412340: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 296 bytes spill stores, 296 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:35.678999: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 900 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:36.180312: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165_0', 988 bytes spill stores, 884 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:36.290820: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:36.576692: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 112 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:36.579226: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 572 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:36.697080: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:37.481642: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:37.513459: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:37.670038: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165_0', 136 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:38.575331: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 900 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:38.912524: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:39.269938: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178_0', 104 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:39.559141: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:39.561693: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 828 bytes spill stores, 1616 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:40.721329: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:41.041218: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_165', 348 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:41.107625: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:41.528251: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 672 bytes spill stores, 648 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:41.924480: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:42.184410: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178_0', 3220 bytes spill stores, 4172 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:42.217547: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 224 bytes spill stores, 224 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:42.348683: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 812 bytes spill stores, 1632 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:43.144350: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:43.829142: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 444 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:44.072576: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59_0', 1288 bytes spill stores, 1288 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:44.409457: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 156 bytes spill stores, 156 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:44.915551: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 4600 bytes spill stores, 4520 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:45.024796: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59_0', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:45.221189: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 420 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:45.966755: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:46.247168: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:46.264145: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 4048 bytes spill stores, 4032 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:46.280355: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_178', 228 bytes spill stores, 180 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:46.671517: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 844 bytes spill stores, 844 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:46.868592: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:47.729653: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:47.979365: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 540 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:48.068104: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_179', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:48.116347: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 600 bytes spill stores, 600 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:48.276633: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 432 bytes spill stores, 432 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:49.104431: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:49.130522: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:49.265892: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:49.708299: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:49.981052: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:50.450526: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 512 bytes spill stores, 512 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:50.457628: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 1764 bytes spill stores, 1736 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:50.855779: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 832 bytes spill stores, 832 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:51.058191: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 832 bytes spill stores, 832 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:51.149356: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 1740 bytes spill stores, 1740 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:51.201312: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:51.315766: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:52.100061: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:52.395496: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 628 bytes spill stores, 628 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:52.490625: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 628 bytes spill stores, 628 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:52.533354: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 1740 bytes spill stores, 1740 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:52.791399: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 452 bytes spill stores, 452 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:53.015270: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:53.015305: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:53.270699: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 452 bytes spill stores, 452 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:53.755710: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:53.895069: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:54.051107: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_180', 484 bytes spill stores, 484 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:54.077388: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 440 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:54.078845: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 512 bytes spill stores, 512 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:54.123957: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:54.833482: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:54.907364: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11218', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2026-01-30 09:46:54.931383: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11145', 440 bytes spill stores, 440 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/1000\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m14:02:29\u001b[0m 51s/step - loss: 0.8287"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1769762826.292490 2369551 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 111ms/step - loss: 0.5282 - val_loss: 0.7020 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 106ms/step - loss: 0.2261 - val_loss: 0.9922 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 106ms/step - loss: 0.1932 - val_loss: 0.9358 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 106ms/step - loss: 0.1842 - val_loss: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 106ms/step - loss: 0.1786 - val_loss: 0.9529 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 09:56:04,859 [INFO] üèÜ Fine-Tuning Completed.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# [ 7. MAIN EXECUTION ]\n",
    "# ====================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Vocabulary\n",
    "    if not os.path.exists(config.VOCAB_PATH):\n",
    "        logger.error(f\"Vocab file not found: {config.VOCAB_PATH}\"); sys.exit(1)\n",
    "    \n",
    "    with open(config.VOCAB_PATH, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    \n",
    "    # 2. Load Model\n",
    "    logger.info(f\"Loading pretrained model from: {config.PRETRAINED_MODEL}\")\n",
    "    try:\n",
    "        model = load_model(\n",
    "            config.PRETRAINED_MODEL, \n",
    "            custom_objects={\n",
    "                \"DynamicPositionalEncoding\": DynamicPositionalEncoding,\n",
    "                \"ImprovedTransformerBlock\": ImprovedTransformerBlock,\n",
    "                \"CustomSchedule\": CustomSchedule,\n",
    "                \"smoothed_loss\": smoothed_loss,\n",
    "            },\n",
    "            compile=False # Load without optimizer state first\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Recompile for Fine-Tuning\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config.LR),\n",
    "        loss=smoothed_loss\n",
    "    )\n",
    "    logger.info(\"Model loaded and recompiled.\")\n",
    "\n",
    "    # 3. Load & Process Data\n",
    "    if not os.path.exists(config.SMILES_FILE):\n",
    "        logger.error(f\"Smiles file not found: {config.SMILES_FILE}\"); sys.exit(1)\n",
    "\n",
    "    with open(config.SMILES_FILE, \"r\") as f:\n",
    "        new_smiles = [l.strip() for l in f if l.strip()]\n",
    "    \n",
    "    processed_data = process_dataset(new_smiles, vocab)\n",
    "    \n",
    "    if len(processed_data) == 0:\n",
    "        logger.error(\"No valid data found after processing. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    train_data, val_data = train_test_split(processed_data, test_size=config.VALID_RATIO, random_state=42)\n",
    "    logger.info(f\"Train size: {len(train_data)} | Val size: {len(val_data)}\")\n",
    "\n",
    "    # 4. Generators & Callbacks\n",
    "    train_gen = CurriculumSmilesGenerator(train_data, vocab)\n",
    "    val_gen = CurriculumSmilesGenerator(val_data, vocab)\n",
    "\n",
    "    run_id = f\"ft_run_{int(time.time())}\"\n",
    "    callbacks = [\n",
    "        CustomTensorBoard(log_dir=f\"logs/{run_id}\"),\n",
    "        EnhancedTrainingMonitor(val_gen),\n",
    "        ModelCheckpoint(\n",
    "            filepath=\"best_finetuned_model.keras\", \n",
    "            monitor=\"val_loss\", \n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 5. Start Training\n",
    "    logger.info(\"üî• Starting Fine-Tuning...\")\n",
    "    try:\n",
    "        model.fit(\n",
    "            train_gen.get_dataset(),\n",
    "            epochs=config.EPOCHS,\n",
    "            steps_per_epoch=config.STEPS_PER_EPOCH,\n",
    "            validation_data=val_gen.get_dataset(),\n",
    "            validation_steps=max(1, len(val_data)//config.BATCH_SIZE),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        model.save(\"final_finetuned_model.keras\")\n",
    "        logger.info(\"üèÜ Fine-Tuning Completed.\")\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"‚ö†Ô∏è Training interrupted.\")\n",
    "        model.save(\"interrupted_finetuned_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd312f-243c-4e78-bd5e-6e9627d345ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfb85e-b303-404c-9c08-19be53af26f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e8eb7-6cd0-4b11-9bc9-4acb4ecb6f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
