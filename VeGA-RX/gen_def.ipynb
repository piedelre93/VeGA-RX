{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3d853e-0908-4c1d-bda2-f9106fb04ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CARICAMENTO MODELLO E VOCABOLARIO\n",
    "# ============================================================================\n",
    "# Paths (MODIFICA QUESTI PERCORSI!)\n",
    "MODEL_PATH = \"/home/grad/Desktop/pietro/denovo/2/final_hybrid_model.keras\"\n",
    "CHAR2IDX_PATH = \"/home/grad/Desktop/pietro/denovo/2/char2idx.pkl\"\n",
    "IDX2CHAR_PATH = \"/home/grad/Desktop/pietro/denovo/2/idx2char.pkl\"\n",
    "VOCAB_PATH = \"/home/grad/Desktop/pietro/denovo/2/vocab.json\"\n",
    "MAX_LENGTH = 140\n",
    "TRAINING_FILE = \"/home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/s4_loro/gen_mio/eval_out_fxr/train.smi\"  # Per novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a61a033-5f9f-441a-a43e-5bfa4dd8befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 12:06:13.218438: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-16 12:06:13.232665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771239973.249867 1255999 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771239973.255097 1255999 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771239973.268343 1255999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771239973.268362 1255999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771239973.268363 1255999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771239973.268365 1255999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-16 12:06:13.272728: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GPU attiva: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "‚úì Vocabolario caricato: 370 token\n",
      "‚Üí Caricamento training set per novelty: /home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/s4_loro/gen_mio/eval_out_fxr/train.smi\n",
      "‚úì 702 SMILES di training caricati\n",
      "‚Üí Caricamento modello da: /home/grad/Desktop/pietro/denovo/2/final_hybrid_model.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1771239975.945674 1255999 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9025 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'improved_transformer_block_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modello caricato e pronto\n"
     ]
    }
   ],
   "source": [
    "##### ============================================================================\n",
    "# GENERAZIONE CONDIZIONATA: SMARTS e/o SCAFFOLD ‚Üí SMILES\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "# Reproducibilit√†\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Configurazione GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(f\"‚úì GPU attiva: {gpus[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö† Errore GPU: {e}\")\n",
    "else:\n",
    "    print(\"‚Üí CPU mode attivo\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURAZIONE\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    EMBED_DIM = 1024\n",
    "    TRANSFORMER_LAYERS = 6\n",
    "    TRANSFORMER_HEADS = 6\n",
    "    FF_DIM = 512\n",
    "    DROPOUT_RATE = 0.1\n",
    "    L2_REG = 1e-4\n",
    "    MAX_LENGTH = 140  # Usa lo stesso valore del training\n",
    "    GRADIENT_CLIP = 1.0\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM OBJECTS (stesse definizioni del training)\n",
    "# ============================================================================\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention, Input\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "def smoothed_loss(y_true, y_pred):\n",
    "    y_true_int = tf.cast(y_true, tf.int32)\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_int, logits=y_pred)\n",
    "    return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        max_len = config.MAX_LENGTH\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"embed_dim\": self.embed_dim}\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim, \n",
    "            kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\", kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)),\n",
    "            Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG))\n",
    "        ])\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.d1 = Dropout(rate)\n",
    "        self.d2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        out1 = self.ln1(inputs + self.d1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.ln2(out1 + self.d2(ffn_output, training=training))\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"embed_dim\": self.embed_dim, \"num_heads\": self.num_heads, \"ffn_dim\": self.ffn_dim, \"rate\": self.rate}\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1.0\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"embed_dim\": float(self.embed_dim.numpy()),\n",
    "            \"warmup_steps\": float(self.warmup_steps.numpy())\n",
    "        }\n",
    "\n",
    "def build_improved_model(vocab_size: int) -> Model:\n",
    "    inputs = Input(shape=(config.MAX_LENGTH,))\n",
    "    x = Embedding(vocab_size, config.EMBED_DIM, mask_zero=False)(inputs)\n",
    "    x = DynamicPositionalEncoding(config.EMBED_DIM)(x)\n",
    "    x = Dropout(config.DROPOUT_RATE)(x)\n",
    "    \n",
    "    for _ in range(config.TRANSFORMER_LAYERS):\n",
    "        x = ImprovedTransformerBlock(\n",
    "            config.EMBED_DIM, \n",
    "            config.TRANSFORMER_HEADS, \n",
    "            config.FF_DIM, \n",
    "            rate=config.DROPOUT_RATE\n",
    "        )(x)\n",
    "        \n",
    "    outputs = Dense(vocab_size)(x)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(\n",
    "        learning_rate=CustomSchedule(config.EMBED_DIM), \n",
    "        clipnorm=config.GRADIENT_CLIP\n",
    "    )\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=opt, loss=smoothed_loss)\n",
    "    return model\n",
    "\n",
    "# ============================================================================\n",
    "# FUNZIONI DI GENERAZIONE\n",
    "# ============================================================================\n",
    "def robust_tokenize(smiles: str) -> list:\n",
    "    pattern = (\n",
    "        r\"(\\[[^\\[\\]]{1,6}\\]|\"\n",
    "        r\"Br|Cl|Si|Na|Mg|Mn|Ca|Fe|Zn|Se|Li|K|Al|B|R[0-9]|r[0-9]|a[0-9]|\"\n",
    "        r\"[A-Za-z0-9@+\\-\\\\\\/\\(\\)=#$.])\"\n",
    "    )\n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    return tokens\n",
    "\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit import Chem\n",
    "\n",
    "def user_scaffold_to_training_murcko_smiles(scaffold_smiles: str):\n",
    "    \"\"\"\n",
    "    Replica la pipeline del training:\n",
    "    1) parse + sanitize\n",
    "    2) Kekulize(clearAromaticFlags=True) sul MOL INTERO\n",
    "    3) MolToSmiles(canonical=True, isomericSmiles=False)\n",
    "    4) rilegge quel SMILES (come nel training)\n",
    "    5) MurckoScaffold.GetScaffoldForMol\n",
    "    6) MolToSmiles(canonical=True, isomericSmiles=False)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(scaffold_smiles, sanitize=True)\n",
    "        if not mol:\n",
    "            return None\n",
    "\n",
    "        # 1) identico a validate_and_fix_smiles\n",
    "        try:\n",
    "            Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        fixed = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "        if not fixed:\n",
    "            return None\n",
    "\n",
    "        # 2) come training: ricarico da fixed\n",
    "        mol2 = Chem.MolFromSmiles(fixed, sanitize=True)\n",
    "        if not mol2:\n",
    "            return None\n",
    "\n",
    "        scaf = MurckoScaffold.GetScaffoldForMol(mol2)\n",
    "        if not scaf:\n",
    "            return None\n",
    "\n",
    "        return Chem.MolToSmiles(scaf, canonical=True, isomericSmiles=False)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def encode_prompt(\n",
    "    char2idx: dict,\n",
    "    smart_names: Optional[List[str]] = None,\n",
    "    scaffold_smiles: Optional[str] = None\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Codifica SMARTS e/o scaffold in token indices per il prompt.\n",
    "    \n",
    "    Args:\n",
    "        char2idx: dizionario token‚Üíindice\n",
    "        smart_names: lista di nomi SMARTS (es: [\"Alcohol\", \"Ketone\"])\n",
    "        scaffold_smiles: stringa SMILES del scaffold\n",
    "        \n",
    "    Returns:\n",
    "        Lista di indici token per il prompt\n",
    "    \"\"\"\n",
    "    tokens = ['<START>']\n",
    "    \n",
    "    if smart_names:\n",
    "        tokens.extend(smart_names)\n",
    "    else:\n",
    "        # Se non ci sono SMARTS, aggiungiamo comunque SEP per formato consistente\n",
    "        pass\n",
    "    \n",
    "    tokens.append('<SEP>')\n",
    "\n",
    "    if scaffold_smiles:\n",
    "        scaffold_smiles = user_scaffold_to_training_murcko_smiles(scaffold_smiles)\n",
    "\n",
    "        if scaffold_smiles:\n",
    "            scaffold_tokens = robust_tokenize(scaffold_smiles)\n",
    "            tokens.extend(scaffold_tokens)\n",
    "            tokens.append('<SEP>')\n",
    "\n",
    "\n",
    "    \n",
    "    # Converti in indici\n",
    "    UNK_IDX = char2idx.get('<UNK>', char2idx['<PAD>'])\n",
    "    return [char2idx.get(t, UNK_IDX) for t in tokens]\n",
    "\n",
    "def generate_smiles_batch_conditioned(model, char2idx, idx2char, max_length, batch_size=64, temperature=1.0,\n",
    "                                      prompt_indices: Optional[List[int]] = None):\n",
    "    PAD_IDX = char2idx['<PAD>']\n",
    "    END_IDX = char2idx['<END>']\n",
    "    \n",
    "    input_seqs = np.full((batch_size, max_length), PAD_IDX, dtype=np.int32)\n",
    "    start_step = 0\n",
    "    \n",
    "    if prompt_indices is None:\n",
    "        input_seqs[:, 0] = char2idx['<START>']\n",
    "        start_step = 1\n",
    "    else:\n",
    "        prompt_len = len(prompt_indices)\n",
    "        if prompt_len >= max_length:\n",
    "             raise ValueError(f\"Prompt troppo lungo ({prompt_len}) rispetto a max_length ({max_length})\")\n",
    "        for i in range(batch_size):\n",
    "            input_seqs[i, :prompt_len] = prompt_indices\n",
    "        start_step = prompt_len\n",
    "\n",
    "    finished = np.zeros(batch_size, dtype=bool)\n",
    "\n",
    "    for t in range(start_step, max_length):\n",
    "        logits = model(input_seqs, training=False)[:, t-1, :]\n",
    "        step_probs = tf.nn.softmax(logits / temperature).numpy()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if not finished[i]:\n",
    "                if np.sum(step_probs[i]) < 1e-6:\n",
    "                    finished[i] = True\n",
    "                    continue\n",
    "                sampled = np.random.choice(len(step_probs[i]), p=step_probs[i])\n",
    "                input_seqs[i, t] = sampled\n",
    "                if sampled == END_IDX:\n",
    "                    finished[i] = True\n",
    "                    \n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "    raw_smiles_list = []\n",
    "    for seq in input_seqs:\n",
    "        tokens = [idx2char.get(idx, '') for idx in seq if idx != PAD_IDX]\n",
    "        sep_indices = [j for j, x in enumerate(tokens) if x == '<SEP>']\n",
    "        \n",
    "        if len(sep_indices) >= 1:\n",
    "            target_tokens = tokens[sep_indices[-1] + 1:]\n",
    "        else:\n",
    "            target_tokens = [t for t in tokens if t != '<START>']\n",
    "\n",
    "        raw_smiles = \"\".join(target_tokens).split('<END>')[0]\n",
    "        raw_smiles_list.append(raw_smiles)\n",
    "\n",
    "    return raw_smiles_list\n",
    "\n",
    "def generate_from_prompt(\n",
    "    model,\n",
    "    char2idx: dict,\n",
    "    idx2char: dict,\n",
    "    max_length: int,\n",
    "    smart_names: Optional[List[str]] = None,\n",
    "    scaffold_smiles: Optional[str] = None,\n",
    "    batch_size: int = 1,\n",
    "    temperature: float = 1.0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Genera SMILES condizionati da SMARTS e/o scaffold.\n",
    "    \n",
    "    Args:\n",
    "        model: modello addestrato\n",
    "        char2idx: dizionario token‚Üíindice\n",
    "        idx2char: dizionario indice‚Üítoken\n",
    "        max_length: lunghezza massima sequenza\n",
    "        smart_names: lista nomi SMARTS (opzionale)\n",
    "        scaffold_smiles: stringa scaffold (opzionale)\n",
    "        batch_size: numero di molecole da generare\n",
    "        temperature: temperatura per sampling\n",
    "        \n",
    "    Returns:\n",
    "        Lista di SMILES generati\n",
    "    \"\"\"\n",
    "    if not smart_names and not scaffold_smiles:\n",
    "        # Generazione incondizionata\n",
    "        prompt_indices = None\n",
    "    else:\n",
    "        # Costruisci prompt\n",
    "        prompt_indices = encode_prompt(char2idx, smart_names, scaffold_smiles)\n",
    "    \n",
    "    return generate_smiles_batch_conditioned(\n",
    "        model, char2idx, idx2char, max_length,\n",
    "        batch_size, temperature, prompt_indices\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Carica vocabolario e dizionari\n",
    "try:\n",
    "    with open(CHAR2IDX_PATH, \"rb\") as f:\n",
    "        char2idx = pickle.load(f)\n",
    "    with open(IDX2CHAR_PATH, \"rb\") as f:\n",
    "        idx2char = pickle.load(f)\n",
    "    with open(VOCAB_PATH, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    print(f\"‚úì Vocabolario caricato: {len(vocab)} token\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore caricamento dizionari: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Carica training set per novelty\n",
    "training_smiles_set = set()\n",
    "if os.path.exists(TRAINING_FILE):\n",
    "    print(f\"‚Üí Caricamento training set per novelty: {TRAINING_FILE}\")\n",
    "    with open(TRAINING_FILE, \"r\") as f:\n",
    "        for line in f:\n",
    "            smi = line.strip()\n",
    "            if smi:\n",
    "                try:\n",
    "                    mol = Chem.MolFromSmiles(smi)\n",
    "                    if mol:\n",
    "                        canon_smi = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "                        training_smiles_set.add(canon_smi)\n",
    "                except: pass\n",
    "    print(f\"‚úì {len(training_smiles_set)} SMILES di training caricati\")\n",
    "else:\n",
    "    print(f\"‚ö† File di training non trovato, novelty disabilitata\")\n",
    "    training_smiles_set = set()\n",
    "\n",
    "# Carica modello\n",
    "custom_objects = {\n",
    "    \"DynamicPositionalEncoding\": DynamicPositionalEncoding,\n",
    "    \"ImprovedTransformerBlock\": ImprovedTransformerBlock,\n",
    "    \"CustomSchedule\": CustomSchedule,\n",
    "    \"smoothed_loss\": smoothed_loss,\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(f\"‚Üí Caricamento modello da: {MODEL_PATH}\")\n",
    "    model = load_model(MODEL_PATH, custom_objects=custom_objects)\n",
    "    model.compile(optimizer='adam', loss=smoothed_loss)\n",
    "    print(\"‚úì Modello caricato e pronto\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore caricamento modello: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# FUNZIONE PER BATCH GENERATION CON REPORT\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# FUNZIONE PER BATCH GENERATION CON REPORT (MODIFICATA)\n",
    "# ============================================================================\n",
    "def evaluate_and_save_batches_conditional(\n",
    "    model, char2idx, idx2char, max_length, training_smiles_set,\n",
    "    smart_names: Optional[List[str]] = None,\n",
    "    scaffold_smiles: Optional[str] = None,\n",
    "    out_csv_path: str = \"generated_conditional.csv\",\n",
    "    num_batches: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    temperature: float = 1.0,\n",
    "    print_progress: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera molecole condizionate, FILTRA per scaffold match, calcola metriche e salva risultati.\n",
    "    \"\"\"\n",
    "    all_raw_generated = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Progress bar\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        iterator = tqdm(range(num_batches), desc=\"Generazione\")\n",
    "    except ImportError:\n",
    "        iterator = range(num_batches)\n",
    "        print(\"Installa tqdm per vedere la progress bar: pip install tqdm\")\n",
    "    \n",
    "    # Genera batch\n",
    "    for b in iterator:\n",
    "        generated_raw = generate_from_prompt(\n",
    "            model, char2idx, idx2char, max_length,\n",
    "            smart_names=smart_names,\n",
    "            scaffold_smiles=scaffold_smiles,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        all_raw_generated.extend(generated_raw)\n",
    "        if not hasattr(iterator, '__len__'):\n",
    "            print(f\"Batch {b+1}/{num_batches}: {len(generated_raw)} SMILES\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_inference_time = end_time - start_time\n",
    "\n",
    "    # Post-processing\n",
    "    if print_progress:\n",
    "        print(\"\\n‚Üí Post-processing (validazione, canonicalization)...\")\n",
    "    \n",
    "    valid_smiles = []\n",
    "    for smi in all_raw_generated:\n",
    "        if not smi: continue\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol:\n",
    "                canon = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "                valid_smiles.append(canon)\n",
    "        except: pass\n",
    "    \n",
    "    # Lista di partenza: molecole valide e uniche\n",
    "    unique_smiles = list(set(valid_smiles))\n",
    "    \n",
    "    # --- NUOVA LOGICA: SCAFFOLD FILTERING ---\n",
    "    scaffold_match_rate = 0.0\n",
    "    filtered_smiles = unique_smiles # Default: se non c'√® scaffold, prendiamo tutto\n",
    "    \n",
    "    if scaffold_smiles:\n",
    "        if print_progress:\n",
    "            print(\"‚Üí Verifica corrispondenza Scaffold...\")\n",
    "            \n",
    "        # 1. Otteniamo la forma canonica \"Murcko\" dello scaffold target\n",
    "        target_scaffold_canon = user_scaffold_to_training_murcko_smiles(scaffold_smiles)\n",
    "        \n",
    "        if target_scaffold_canon:\n",
    "            matching_smiles = []\n",
    "            for smi in unique_smiles:\n",
    "                # 2. Estraiamo lo scaffold dalla molecola generata con la stessa funzione\n",
    "                gen_scaffold = user_scaffold_to_training_murcko_smiles(smi)\n",
    "                \n",
    "                # 3. Controllo identit√† stringa\n",
    "                if gen_scaffold == target_scaffold_canon:\n",
    "                    matching_smiles.append(smi)\n",
    "            \n",
    "            # Calcolo statistiche\n",
    "            if len(unique_smiles) > 0:\n",
    "                scaffold_match_rate = len(matching_smiles) / len(unique_smiles)\n",
    "            \n",
    "            # Sovrascriviamo la lista da analizzare e salvare\n",
    "            filtered_smiles = matching_smiles\n",
    "            \n",
    "            if print_progress:\n",
    "                print(f\"‚úì Target Scaffold: {target_scaffold_canon}\")\n",
    "                print(f\"‚úì Match Rate: {len(filtered_smiles)}/{len(unique_smiles)} ({scaffold_match_rate*100:.2f}%)\")\n",
    "        else:\n",
    "            if print_progress:\n",
    "                print(\"‚ö† Attenzione: Impossibile calcolare Murcko Scaffold dall'input fornito. Salto il filtro.\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Calcolo propriet√† (SUI FILTRATI)\n",
    "    qed_list, sa_list = [], []\n",
    "    try:\n",
    "        from rdkit.Chem import RDConfig\n",
    "        sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "        import sascorer\n",
    "        HAS_SA = True\n",
    "    except:\n",
    "        HAS_SA = False\n",
    "        if print_progress:\n",
    "            print(\"‚ö† SA_Score non disponibile\")\n",
    "\n",
    "    for smi in filtered_smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            try: \n",
    "                qed_list.append(QED.qed(mol))\n",
    "            except: pass\n",
    "            if HAS_SA:\n",
    "                try: \n",
    "                    sa_list.append(sascorer.calculateScore(mol))\n",
    "                except: pass\n",
    "\n",
    "    # Salva risultati (SOLO NOVEL E FILTRATI)\n",
    "    only_novel_smiles = [smi for smi in filtered_smiles if smi not in training_smiles_set]\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv_path) if os.path.dirname(out_csv_path) else '.', exist_ok=True)\n",
    "    with open(out_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"SMILES\"]) # Header opzionale, rimuovilo se preferisci file raw\n",
    "        for smi in only_novel_smiles:\n",
    "            writer.writerow([smi])\n",
    "\n",
    "\n",
    "    # Metriche Finali\n",
    "    total_generated_count = len(all_raw_generated)\n",
    "    validity = len(valid_smiles) / total_generated_count if total_generated_count else 0\n",
    "    uniqueness = len(unique_smiles) / len(valid_smiles) if valid_smiles else 0\n",
    "    \n",
    "    # Novelty calcolata sui filtrati\n",
    "    novelty_rate = len(only_novel_smiles) / len(filtered_smiles) if filtered_smiles else 0\n",
    "    \n",
    "    avg_qed = np.mean(qed_list) if qed_list else 0\n",
    "    avg_sa = np.mean(sa_list) if sa_list else 0\n",
    "\n",
    "    time_per_batch = total_inference_time / num_batches if num_batches > 0 else 0\n",
    "    time_per_molecule = total_inference_time / total_generated_count if total_generated_count else 0\n",
    "    \n",
    "    # Report condizionato\n",
    "    condition_desc = []\n",
    "    if smart_names:\n",
    "        condition_desc.append(f\"SMARTS={smart_names}\")\n",
    "    if scaffold_smiles:\n",
    "        condition_desc.append(f\"SCAFFOLD={scaffold_smiles}\")\n",
    "    condition_str = \" | \".join(condition_desc) if condition_desc else \"Incondizionata\"\n",
    "    \n",
    "    scaffold_info_str = \"\"\n",
    "    if scaffold_smiles:\n",
    "        scaffold_info_str = f\"  Scaffold Match Rate:      {scaffold_match_rate*100:.2f}% (su {len(unique_smiles)} unici)\"\n",
    "\n",
    "    if print_progress:\n",
    "        print(f\"\"\"\n",
    "{'='*70}\n",
    "üìä GENERATION REPORT - {condition_str}\n",
    "{'='*70}\n",
    "  Total generated (raw):    {total_generated_count:,}\n",
    "  Valid SMILES:             {len(valid_smiles):,} ({validity*100:.2f}%)\n",
    "  Unique SMILES:            {len(unique_smiles):,} (uniqueness: {uniqueness*100:.2f}%)\n",
    "{scaffold_info_str}\n",
    "  Saved (Filtered & Novel): {len(only_novel_smiles):,}\n",
    "  Novelty (on filtered):    {novelty_rate*100:.2f}%\n",
    "  Average QED (filtered):   {avg_qed:.4f}\n",
    "  Average SA (filtered):    {avg_sa:.4f}\n",
    "  \n",
    "‚è±Ô∏è  INFERENCE TIME\n",
    "  Total time:               {total_inference_time:.2f} seconds\n",
    "  Time per batch:           {time_per_batch:.3f} seconds\n",
    "  Time per molecule:        {time_per_molecule*1000:.2f} ms\n",
    "{'='*70}\n",
    "\"\"\")\n",
    "    \n",
    "    return {\n",
    "        \"total_input\": total_generated_count,\n",
    "        \"valid_count\": len(valid_smiles),\n",
    "        \"validity_rate\": validity,\n",
    "        \"unique_count\": len(unique_smiles),\n",
    "        \"uniqueness_rate\": uniqueness,\n",
    "        \"scaffold_match_rate\": scaffold_match_rate,\n",
    "        \"novelty_rate\": novelty_rate,\n",
    "        \"avg_qed\": avg_qed,\n",
    "        \"avg_sa\": avg_sa,\n",
    "        \"output_file\": out_csv_path\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661a701-4ab1-495b-902d-a477bfe4729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONDITIONAL GENERATION: SMARTS and/or SCAFFOLD ‚Üí SMILES\n",
      "======================================================================\n",
      "\n",
      "--- Menu ---\n",
      "1. Generate from SMARTS\n",
      "2. Generate from SCAFFOLD\n",
      "3. Generate from SMARTS + SCAFFOLD\n",
      "4. Unconditional generation\n",
      "5. BATCH generation with full report\n",
      "0. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose an option (0-5):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Multi-Batch Generation with Report ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Molecules per batch (default 64):  10\n",
      "Number of batches (default 10):  10\n",
      "Temperature (default 1.0):  1\n",
      "Use conditioning? (y/n, default n):  n\n",
      "Output file path (default ./generated_batch.csv):  ok.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generazione:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 2/10 [00:20<01:20, 10.09s/it]"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INTERACTIVE USAGE EXAMPLES\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONDITIONAL GENERATION: SMARTS and/or SCAFFOLD ‚Üí SMILES\")\n",
    "    print(\"=\"*70)\n",
    "    from rdkit import RDLogger\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "    while True:\n",
    "        print(\"\\n--- Menu ---\")\n",
    "        print(\"1. Generate from SMARTS\")\n",
    "        print(\"2. Generate from SCAFFOLD\")\n",
    "        print(\"3. Generate from SMARTS + SCAFFOLD\")\n",
    "        print(\"4. Unconditional generation\")\n",
    "        print(\"5. BATCH generation with full report\")\n",
    "        print(\"0. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nChoose an option (0-5): \").strip()\n",
    "        \n",
    "        if choice == \"0\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        \n",
    "        # Option 5: Batch generation with report\n",
    "        if choice == \"5\":\n",
    "            print(\"\\n--- Multi-Batch Generation with Report ---\")\n",
    "            batch_size = int(input(\"Molecules per batch (default 64): \").strip() or \"64\")\n",
    "            num_batches = int(input(\"Number of batches (default 10): \").strip() or \"10\")\n",
    "            temperature = float(input(\"Temperature (default 1.0): \").strip() or \"1.0\")\n",
    "            \n",
    "            smart_names = None\n",
    "            scaffold_smiles = None\n",
    "            \n",
    "            # Ask whether to use conditioning\n",
    "            use_condition = input(\"Use conditioning? (y/n, default n): \").strip().lower()\n",
    "            if use_condition == 's':\n",
    "                cond_type = input(\"Type (smarts/scaffold/both): \").strip().lower()\n",
    "                if cond_type in ['smarts', 'both']:\n",
    "                    smart_input = input(\"Enter SMARTS names separated by comma: \").strip()\n",
    "                    if smart_input:\n",
    "                        smart_names = [s.strip() for s in smart_input.split(',')]\n",
    "                if cond_type in ['scaffold', 'both']:\n",
    "                    scaffold_smiles = input(\"Enter scaffold SMILES: \").strip()\n",
    "            \n",
    "            output_file = input(\"Output file path (default ./generated_batch.csv): \").strip() or \"generated_batch.csv\"\n",
    "            \n",
    "            # Run batch generation\n",
    "            metrics = evaluate_and_save_batches_conditional(\n",
    "                model, char2idx, idx2char, MAX_LENGTH, training_smiles_set,\n",
    "                smart_names=smart_names,\n",
    "                scaffold_smiles=scaffold_smiles,\n",
    "                out_csv_path=output_file,\n",
    "                num_batches=num_batches,\n",
    "                batch_size=batch_size,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n‚úì Generation completed!\")\n",
    "            continue\n",
    "        \n",
    "        # Options 1-4: Single/interactive generation\n",
    "        batch_size = int(input(\"Number of molecules to generate (1-20): \").strip())\n",
    "        temperature = float(input(\"Temperature (0.1-2.0, default 1.0): \").strip() or \"1.0\")\n",
    "        \n",
    "        smart_names = None\n",
    "        scaffold_smiles = None\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            smart_input = input(\"Enter SMARTS names separated by comma (e.g.: Alcohol,Ketone): \").strip()\n",
    "            if smart_input:\n",
    "                smart_names = [s.strip() for s in smart_input.split(',')]\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            scaffold_smiles = input(\"Enter scaffold SMILES (e.g.: c1ccccc1): \").strip()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            smart_input = input(\"Enter SMARTS names separated by comma (e.g.: Alcohol,Ketone): \").strip()\n",
    "            if smart_input:\n",
    "                smart_names = [s.strip() for s in smart_input.split(',')]\n",
    "            scaffold_smiles = input(\"Enter scaffold SMILES (e.g.: c1ccccc1): \").strip()\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid option!\")\n",
    "            continue\n",
    "        \n",
    "        # Generate\n",
    "        print(\"\\n‚Üí Generating...\")\n",
    "        generated_smiles = generate_from_prompt(\n",
    "            model, char2idx, idx2char, MAX_LENGTH,\n",
    "            smart_names=smart_names,\n",
    "            scaffold_smiles=scaffold_smiles,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n‚úì Results:\")\n",
    "        for i, smi in enumerate(generated_smiles, 1):\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol:\n",
    "                qed_val = QED.qed(mol) if mol else 0\n",
    "                print(f\"  {i}. {smi} (QED: {qed_val:.3f})\")\n",
    "            else:\n",
    "                print(f\"  {i}. {smi} (INVALID)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
