{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a25788-e326-4635-b49c-6e339329e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# [ 1. CONFIGURAZIONE ]\n",
    "# ====================================================================\n",
    "\n",
    "class Config:\n",
    "    # Percorsi file\n",
    "    SMILES_FILE = \"/home/grad/Desktop/pietro/denovo/2/compacted_smiles.txt\"\n",
    "    SMARTS_RX_FILE = \"/home/grad/Desktop/pietro/denovo/2/SMART_RX/smartsrx-main/smartsrx.json\"\n",
    "    \n",
    "    VOCAB_PATH = \"/home/grad/Desktop/pietro/denovo/2/smartrx/vocab.json\"\n",
    "    # Iperparametri Modello\n",
    "    EMBED_DIM = 512\n",
    "    TRANSFORMER_LAYERS = 6\n",
    "    TRANSFORMER_HEADS = 6\n",
    "    FF_DIM = 2048\n",
    "    DROPOUT_RATE = 0.10\n",
    "    L2_REG = 1e-4\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 400\n",
    "    MAX_LENGTH = 100         # Adatto per SMARTS + SMILES\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    VALID_RATIO = 0.1\n",
    "    \n",
    "    # Curriculum & Augmentation\n",
    "    CURRICULUM_START_COMPLEXITY = 10\n",
    "    CURRICULUM_COMPLEXITY_STEP = 5\n",
    "    LOSS_STABILITY_THRESHOLD = 0.01\n",
    "    WARMUP_EPOCHS = 5\n",
    "    AUGMENT_PROB = 0.10\n",
    "    \n",
    "    # Generazione\n",
    "    TEMPERATURE = 1.0\n",
    "    GEN_NUM = 10\n",
    "    PRINT_EVERY = 100\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "config = Config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72c949-1d99-4347-ac0e-f4ce9f458934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 12:20:04.478642: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-16 12:20:04.492613: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771240804.509778 1263289 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771240804.515122 1263289 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771240804.528501 1263289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771240804.528519 1263289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771240804.528521 1263289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771240804.528522 1263289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-16 12:20:04.532834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING: SMARTS-RX â†’ SMILES (NO SCAFFOLD)\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from threading import Lock\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# RDKit Imports\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogEntry, SmartsMatcher \n",
    "from rdkit.Chem import FilterCatalog  # Necessario per SMARTS-RX\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow Imports\n",
    "from tensorflow.keras.layers import Layer, Embedding, Input, LayerNormalization, MultiHeadAttention, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING: SMARTS-RX â†’ SMILES (SENZA SCAFFOLD)\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from threading import Lock\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# RDKit Imports\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogEntry, SmartsMatcher\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow Importsnew/cam\n",
    "from tensorflow.keras.layers import Layer, Embedding, Input, LayerNormalization, MultiHeadAttention, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Configurazione Logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "# ====================================================================\n",
    "# [ 2. GESTIONE SMARTS-RX ]\n",
    "# ====================================================================\n",
    "\n",
    "SMARTS_CATALOG = None\n",
    "\n",
    "def initialize_smarts_catalog():\n",
    "    \"\"\"Carica il catalogo SMARTS-RX.\"\"\"\n",
    "    global SMARTS_CATALOG\n",
    "    if SMARTS_CATALOG is not None: return\n",
    "\n",
    "    try:\n",
    "        with open(config.SMARTS_RX_FILE, \"rt\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Errore caricamento SMARTS JSON: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    catalog = FilterCatalog()\n",
    "    count = 0\n",
    "    for entry in data.get(\"data\", []):\n",
    "        name = entry.get(\"specific_type\")\n",
    "        smarts = entry.get(\"smarts\")\n",
    "        if name and smarts:\n",
    "            pattern = Chem.MolFromSmarts(smarts)\n",
    "            if pattern:\n",
    "                catalog.AddEntry(FilterCatalogEntry(name, SmartsMatcher(pattern)))\n",
    "                count += 1\n",
    "    SMARTS_CATALOG = catalog\n",
    "    logger.info(f\"âœ“ Catalogo SMARTS-RX caricato: {count} regole.\")\n",
    "\n",
    "def get_smarts_fingerprint(mol: Chem.Mol) -> List[str]:\n",
    "    \"\"\"Restituisce lista nomi gruppi funzionali presenti.\"\"\"\n",
    "    if not mol or SMARTS_CATALOG is None: return []\n",
    "    matches = SMARTS_CATALOG.GetMatches(mol)\n",
    "    return sorted(list(set([m.GetDescription() for m in matches])))\n",
    "\n",
    "# ====================================================================\n",
    "# [ 3. HELPER CHIMICI ]\n",
    "# ====================================================================\n",
    "def robust_tokenize(smiles: str) -> list:\n",
    "    # PRIORITÃ€ ASSOLUTA: Atomi aromatici (c, n, o) prima delle maiuscole\n",
    "    pattern = (\n",
    "        r\"(\\[[^\\[\\]]{1,6}\\]|\"                 # 1. Atomi tra parentesi [C@@H]\n",
    "        r\"Br|Cl|Si|Na|Mg|Mn|Ca|Fe|Zn|Se|Li|K|Al|B|\"  # 2. Elementi specifici (2 lettere)\n",
    "        r\"c|n|o|s|p|\"                         # 3. ğŸš¨ FIX: Atomi aromatici (1 lettera minuscola)\n",
    "        r\"R[0-9]|r[0-9]|a[0-9]|\"              # 4. Ring labels\n",
    "        r\"[A-Za-z0-9@+\\-\\\\\\/\\(\\)=#\\$\\.\\%,])\"  # 5. Tutto il resto\n",
    "    )\n",
    "    \n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    \n",
    "    # Controllo parentesi (stack) semplificato\n",
    "    stack = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('['): stack.append(t)\n",
    "        if t.endswith(']'):\n",
    "            if not stack: return [] # Parentesi chiusa senza aperta\n",
    "            stack.pop()\n",
    "    \n",
    "    # Se lo stack non Ã¨ vuoto, ci sono parentesi non chiuse\n",
    "    if stack: return []\n",
    "    \n",
    "    # Check finale: se unendo i token non esce una molecola valida, scarta\n",
    "    try:\n",
    "        if not Chem.MolFromSmiles(''.join(tokens)):\n",
    "            return []\n",
    "    except:\n",
    "        return []\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def validate_and_fix_smiles(smiles: str) -> str:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is None: return None\n",
    "        try: Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        except: pass\n",
    "        return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "    except: return None\n",
    "\n",
    "def randomize_smiles(smiles: str, num_versions: int = 1) -> List[str]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol: return []\n",
    "    res = []\n",
    "    try:\n",
    "        # FIX: Forziamo la Kekulizzazione per coerenza con il vocabolario\n",
    "        Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        # Generiamo SMILES random ma mantenendo la forma di KekulÃ©\n",
    "        s = Chem.MolToSmiles(mol, doRandom=True, canonical=False, kekuleSmiles=True)\n",
    "        if s: res.append(s)\n",
    "    except: pass\n",
    "    return res\n",
    "\n",
    "def compute_complexity_from_tokens(tokens: List[str]) -> int:\n",
    "    smiles = ''.join(tokens)\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol: return 999\n",
    "        return Chem.GetSSSR(mol) + smiles.count('(')\n",
    "    except: return 999\n",
    "\n",
    "# ====================================================================\n",
    "# [ 4. PREPROCESSING DATASET (SMARTS + SMILES) ]\n",
    "# ====================================================================\n",
    "\n",
    "def process_dataset(data: List[str]) -> Tuple[List[Tuple[List[str], List[str]]], List[str], int]:\n",
    "    \"\"\"\n",
    "    Output Tuple: (SMARTS_TOKENS, TARGET_TOKENS)\n",
    "    SMARTS_TOKENS: lista di nomi gruppi funzionali\n",
    "    TARGET_TOKENS: lista di token SMILES\n",
    "    \"\"\"\n",
    "    initialize_smarts_catalog()\n",
    "    \n",
    "    processed = []\n",
    "    all_tokens = set()\n",
    "    \n",
    "    logger.info(\"â†’ Inizio Preprocessing (SMARTS-RX + SMILES)...\")\n",
    "    \n",
    "    for idx, s in enumerate(data):\n",
    "        if idx % 10000 == 0: logger.info(f\"  Processati {idx}/{len(data)}\")\n",
    "            \n",
    "        fixed = validate_and_fix_smiles(s)\n",
    "        if not fixed: continue\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(fixed)\n",
    "        if not mol: continue\n",
    "\n",
    "        # 1. Target (SMILES) â†’ Tokenizza\n",
    "        tokens_target = robust_tokenize(fixed)\n",
    "        if not tokens_target: continue\n",
    "        \n",
    "        # 2. SMARTS-RX â†’ Lista di nomi (NON tokenizzare!)\n",
    "        smarts_names = get_smarts_fingerprint(mol)\n",
    "        \n",
    "        # STRUTTURA CORRETTA: (['Alcohol', 'Ketone'], ['C', 'C', '=', 'O', ...])\n",
    "        total_len = 1 + len(smarts_names) + 1 + len(tokens_target) + 1\n",
    "\n",
    "        if 4 <= total_len <= config.MAX_LENGTH:\n",
    "            processed.append((smarts_names, tokens_target))  # âœ… CORRETTO\n",
    "            all_tokens.update(tokens_target)\n",
    "            all_tokens.update(smarts_names)\n",
    "\n",
    "    # Costruzione Vocabolario\n",
    "    vocab = ['<PAD>', '<START>', '<END>', '<SEP>']\n",
    "    vocab.extend(sorted(list(all_tokens)))\n",
    "    \n",
    "    # Ricalcolo Max Len\n",
    "    lengths = []\n",
    "    for smarts_tokens, target_tokens in processed:\n",
    "        lengths.append(1 + len(smarts_tokens) + 1 + len(target_tokens) + 1)\n",
    "        \n",
    "    if processed:\n",
    "        max_len = min(int(np.percentile(lengths, 99)), config.MAX_LENGTH)\n",
    "    else:\n",
    "        max_len = config.MAX_LENGTH\n",
    "    \n",
    "    logger.info(f\"âœ“ Preprocessing completato: {len(processed)} molecole | Vocab: {len(vocab)} | MaxLen: {max_len}\")\n",
    "    return processed, vocab, max_len\n",
    "\n",
    "# ====================================================================\n",
    "# [ 5. CLASSI THREAD-SAFE ]\n",
    "# ====================================================================\n",
    "\n",
    "class ThreadSafeIterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = Lock()\n",
    "    def __iter__(self): return self\n",
    "    def __next__(self):\n",
    "        with self.lock: return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    def wrapper(*args, **kwargs): return ThreadSafeIterator(func(*args, **kwargs))\n",
    "    return wrapper\n",
    "\n",
    "# ====================================================================\n",
    "# [ 6. GENERATORE CURRICULUM (2 PARTI) ]\n",
    "# ====================================================================\n",
    "\n",
    "class CurriculumSmilesGenerator:\n",
    "    \"\"\"\n",
    "    Generatore di sequenze per training con curriculum learning.\n",
    "    Formato: [START] SMARTS [SEP] TARGET [END]\n",
    "    Nessun token <UNK> - tutti i token devono essere nel vocabolario.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processed_data: List[Tuple[List[str], List[str]]], vocab: List[str]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            processed_data: Lista di tuple (smart_names, target_tokens)\n",
    "            vocab: Lista completa di token nel vocabolario\n",
    "        \"\"\"\n",
    "        # Mappature token â†” indice\n",
    "        self.char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        \n",
    "        # Verifica token essenziali\n",
    "        essential_tokens = ['<PAD>', '<START>', '<END>', '<SEP>']\n",
    "        for token in essential_tokens:\n",
    "            if token not in self.char2idx:\n",
    "                raise ValueError(f\"Token essenziale '{token}' mancante nel vocabolario!\")\n",
    "        \n",
    "        # Dati originali con complessitÃ \n",
    "        self.original_data = []\n",
    "        for smarts, target in processed_data:\n",
    "            comp = compute_complexity_from_tokens(target)\n",
    "            self.original_data.append(((smarts, target), comp))\n",
    "        \n",
    "        # Parametri curriculum\n",
    "        valid_comps = [c for _, c in self.original_data if c != 999]\n",
    "        self.max_complexity = max(valid_comps) if valid_comps else 0\n",
    "        self.current_complexity = config.CURRICULUM_START_COMPLEXITY\n",
    "        self.available_data = self._filter_data()\n",
    "        \n",
    "        # Per novelty check\n",
    "        self.train_smiles = {''.join(target) for (_, target), _ in self.original_data}\n",
    "        \n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def _filter_data(self) -> List[Tuple[Tuple[List[str], List[str]], int]]:\n",
    "        \"\"\"Filtra i dati per complessitÃ  <= current_complexity.\"\"\"\n",
    "        filtered = [dp for dp, c in self.original_data if c <= self.current_complexity]\n",
    "        return filtered if filtered else [dp for dp, _ in self.original_data]\n",
    "    \n",
    "    def update_complexity(self, epoch: int, loss_diff: float = None):\n",
    "        \"\"\"\n",
    "        Aggiorna la complessitÃ  del curriculum.\n",
    "        Args:\n",
    "            epoch: Epoca corrente\n",
    "            loss_diff: Differenza di loss (se None, usa strategia warmup)\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            if loss_diff is not None and loss_diff < config.LOSS_STABILITY_THRESHOLD:\n",
    "                # Aumenta complessitÃ  se la loss Ã¨ stabile\n",
    "                self.current_complexity = min(\n",
    "                    self.current_complexity + config.CURRICULUM_COMPLEXITY_STEP, \n",
    "                    self.max_complexity\n",
    "                )\n",
    "            else:\n",
    "                # Strategia warmup\n",
    "                if epoch <= config.WARMUP_EPOCHS:\n",
    "                    incr = int((self.max_complexity - config.CURRICULUM_START_COMPLEXITY) * \n",
    "                              (epoch / config.WARMUP_EPOCHS))\n",
    "                    self.current_complexity = config.CURRICULUM_START_COMPLEXITY + incr\n",
    "                else:\n",
    "                    self.current_complexity = self.max_complexity\n",
    "            \n",
    "            self.available_data = self._filter_data()\n",
    "            if not self.available_data:\n",
    "                self.available_data = [dp for dp, _ in self.original_data]\n",
    "                logger.warning(\"Reset available_data (fallback).\")\n",
    "    \n",
    "    @threadsafe_generator\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Genera batch di (inputs, targets) per il training.\n",
    "        Yield: (inputs, targets) dove targets[i] = inputs[i][1:] + <PAD>\n",
    "        \"\"\"\n",
    "        PAD_IDX = self.char2idx['<PAD>']\n",
    "        START_IDX = self.char2idx['<START>']\n",
    "        END_IDX = self.char2idx['<END>']\n",
    "        SEP_IDX = self.char2idx['<SEP>']\n",
    "\n",
    "        while True:\n",
    "            inputs = np.full((config.BATCH_SIZE, config.MAX_LENGTH), PAD_IDX, dtype=np.int32)\n",
    "            targets = np.full_like(inputs, PAD_IDX)\n",
    "            \n",
    "            for i in range(config.BATCH_SIZE):\n",
    "                # Seleziona esempio dal dataset corrente\n",
    "                with self.lock:\n",
    "                    try:\n",
    "                        data_pair = random.choice(self.available_data)\n",
    "                    except:\n",
    "                        self.available_data = [dp for dp, _ in self.original_data]\n",
    "                        data_pair = random.choice(self.available_data)\n",
    "                \n",
    "                # Unpacking: (smart_names, target_tokens)\n",
    "                smarts_names, target_tokens = data_pair\n",
    "\n",
    "                # Augmentation solo sul target\n",
    "                curr_target = target_tokens\n",
    "                if random.random() < config.AUGMENT_PROB:\n",
    "                    try:\n",
    "                        aug = randomize_smiles(''.join(target_tokens), 1)\n",
    "                        if aug:\n",
    "                            tok = robust_tokenize(aug[0])\n",
    "                            if tok: curr_target = tok\n",
    "                    except: pass\n",
    "\n",
    "                # Costruisci sequenza completa\n",
    "                # Formato: [START] SMARTS [SEP] TARGET [END]\n",
    "                seq = (['<START>'] + \n",
    "                       smarts_names + \n",
    "                       ['<SEP>'] + \n",
    "                       curr_target + \n",
    "                       ['<END>'])\n",
    "                \n",
    "                # Padding\n",
    "                padded_seq = (seq + ['<PAD>'] * config.MAX_LENGTH)[:config.MAX_LENGTH]\n",
    "                \n",
    "                # Converti in indici (tutti i token sono garantiti esistere)\n",
    "                indices = [self.char2idx[t] for t in padded_seq]\n",
    "                \n",
    "                # Assegna input e target\n",
    "                inputs[i] = indices\n",
    "                targets[i, :-1] = indices[1:]  # Shift di 1\n",
    "                targets[i, -1] = PAD_IDX       # Ultima posizione padding\n",
    "                \n",
    "            yield inputs, targets\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"Restituisce un tf.data.Dataset pronto per il training.\"\"\"\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            self.__call__,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32)\n",
    "            )\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ====================================================================\n",
    "# [ 7. MODELLO TRANSFORMER ]\n",
    "# ====================================================================\n",
    "\n",
    "# 1. DEFINIZIONE LOSS\n",
    "def smoothed_loss(y_true, y_pred):\n",
    "    y_true_int = tf.cast(y_true, tf.int32)\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_int, logits=y_pred)\n",
    "    return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "# 2. LAYERS CUSTOM\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        max_len = config.MAX_LENGTH\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"embed_dim\": self.embed_dim}\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim, \n",
    "            kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\", kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)),\n",
    "            Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG))\n",
    "        ])\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.d1 = Dropout(rate)\n",
    "        self.d2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        out1 = self.ln1(inputs + self.d1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.ln2(out1 + self.d2(ffn_output, training=training))\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"embed_dim\": self.embed_dim, \"num_heads\": self.num_heads, \"ffn_dim\": self.ffn_dim, \"rate\": self.rate}\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super().__init__()\n",
    "        # FIX: Usa tf.cast per evitare errori di tipo\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": float(self.embed_dim), \"warmup_steps\": float(self.warmup_steps)}\n",
    "# 3. COSTRUZIONE MODELLO\n",
    "def build_improved_model(vocab_size: int) -> Model:\n",
    "    inputs = Input(shape=(config.MAX_LENGTH,))\n",
    "    x = Embedding(vocab_size, config.EMBED_DIM, mask_zero=False)(inputs)\n",
    "    x = DynamicPositionalEncoding(config.EMBED_DIM)(x)\n",
    "    x = Dropout(config.DROPOUT_RATE)(x)\n",
    "    \n",
    "    for _ in range(config.TRANSFORMER_LAYERS):\n",
    "        x = ImprovedTransformerBlock(\n",
    "            config.EMBED_DIM, \n",
    "            config.TRANSFORMER_HEADS, \n",
    "            config.FF_DIM, \n",
    "            rate=config.DROPOUT_RATE\n",
    "        )(x)\n",
    "        \n",
    "    outputs = Dense(vocab_size)(x)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(\n",
    "        learning_rate=CustomSchedule(config.EMBED_DIM), \n",
    "        clipnorm=config.GRADIENT_CLIP\n",
    "    )\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=opt, loss=smoothed_loss)\n",
    "    return model\n",
    "\n",
    "# ====================================================================\n",
    "# [ 8. MONITORAGGIO ]\n",
    "# ====================================================================\n",
    "\n",
    "class CustomTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}; lr = self.model.optimizer.learning_rate\n",
    "        logs['lr'] = lr(epoch).numpy() if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule) else lr.numpy()\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class EnhancedTrainingMonitor(Callback):\n",
    "    def __init__(self, val_gen: CurriculumSmilesGenerator):\n",
    "        super().__init__()\n",
    "        self.val_gen = val_gen\n",
    "        self.best_val_loss = np.inf; self.prev_val_loss = None\n",
    "\n",
    "    def generate_sample(self, num: int):\n",
    "        generated, valid = [], []\n",
    "        PAD, START, END = [self.val_gen.char2idx[k] for k in ['<PAD>','<START>','<END>']]\n",
    "        input_seq = np.full((1, config.MAX_LENGTH), PAD, dtype=np.int32)\n",
    "        \n",
    "        for _ in range(num):\n",
    "            input_seq.fill(PAD); input_seq[0, 0] = START\n",
    "            for t in range(1, config.MAX_LENGTH):\n",
    "                logits = self.model(input_seq, training=False)[0, t-1]\n",
    "                probs = tf.nn.softmax(logits / config.TEMPERATURE).numpy()\n",
    "                if np.sum(probs) < 1e-6: break\n",
    "                sampled = np.random.choice(len(probs), p=probs)\n",
    "                input_seq[0, t] = sampled\n",
    "                if sampled == END: break\n",
    "            \n",
    "            indices = input_seq[0].tolist()\n",
    "            raw_tokens = [self.val_gen.idx2char[i] for i in indices if i not in {PAD, START}]\n",
    "            \n",
    "            # Parsing: SMARTS [SEP] TARGET (una sola occorrenza di SEP)\n",
    "            try:\n",
    "                sep_idx = raw_tokens.index('<SEP>')\n",
    "                target_tokens = raw_tokens[sep_idx + 1:]\n",
    "                smi_str = \"\".join(target_tokens).split('<END>')[0]\n",
    "            except (ValueError, IndexError):\n",
    "                smi_str = \"\".join(raw_tokens).split('<END>')[0]\n",
    "            \n",
    "            final = validate_and_fix_smiles(smi_str)\n",
    "            if final: valid.append(final)\n",
    "            generated.append(final or smi_str)\n",
    "            \n",
    "        return generated, valid\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if self.prev_val_loss and 'val_loss' in logs:\n",
    "            diff = (self.prev_val_loss - logs['val_loss']) / self.prev_val_loss\n",
    "            self.val_gen.update_complexity(epoch, diff)\n",
    "        if logs.get('val_loss', np.inf) < self.best_val_loss: self.best_val_loss = logs['val_loss']\n",
    "        self.prev_val_loss = logs.get('val_loss')\n",
    "        \n",
    "        if (epoch + 1) % config.PRINT_EVERY == 0:\n",
    "            gen, val = self.generate_sample(config.GEN_NUM)\n",
    "            validity = len(val) / config.GEN_NUM\n",
    "            novel = len([s for s in val if s not in self.val_gen.train_smiles])\n",
    "            logger.info(f\"\\nEPOCH {epoch+1}: Validity {validity:.1%} | Novelty {novel}/{len(val)}\")\n",
    "            if val: logger.info(f\"Sample: {val[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aded2ca-6c42-4cd1-857b-78640412ef0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 12:19:01,607 [INFO] ğŸš€ START TRAINING: SMARTS-RX â†’ SMILES\n",
      "2026-02-16 12:19:01,810 [INFO] âœ“ Caricati 891004 SMILES.\n",
      "2026-02-16 12:19:01,862 [INFO] âœ“ Catalogo SMARTS-RX caricato: 406 regole.\n",
      "2026-02-16 12:19:01,863 [INFO] â†’ Inizio Preprocessing (SMARTS-RX + SMILES)...\n",
      "2026-02-16 12:19:01,863 [INFO]   Processati 0/891004\n",
      "[12:19:01] SMILES Parse Error: syntax error while parsing: SMILES\n",
      "[12:19:01] SMILES Parse Error: Failed parsing SMILES 'SMILES' for input: 'SMILES'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Caricati \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(raw_smiles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m SMILES.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 2. Preprocessing\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m processed, vocab, max_len \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_smiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m config\u001b[38;5;241m.\u001b[39mMAX_LENGTH \u001b[38;5;241m=\u001b[39m max_len\n\u001b[1;32m     21\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Vocab: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vocab)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m token | MaxLen: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 192\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tokens_target: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# 2. SMARTS-RX â†’ Lista di nomi (NON tokenizzare!)\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m smarts_names \u001b[38;5;241m=\u001b[39m \u001b[43mget_smarts_fingerprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# STRUTTURA CORRETTA: (['Alcohol', 'Ketone'], ['C', 'C', '=', 'O', ...])\u001b[39;00m\n\u001b[1;32m    195\u001b[0m total_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(smarts_names) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens_target) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m, in \u001b[0;36mget_smarts_fingerprint\u001b[0;34m(mol)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Restituisce lista nomi gruppi funzionali presenti.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mol \u001b[38;5;129;01mor\u001b[39;00m SMARTS_CATALOG \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m---> 93\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mSMARTS_CATALOG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetMatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m([m\u001b[38;5;241m.\u001b[39mGetDescription() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matches])))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# [ 9. MAIN EXECUTION ]\n",
    "# ====================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"ğŸš€ START TRAINING: SMARTS-RX â†’ SMILES\")\n",
    "    \n",
    "    # 1. Verifica Files\n",
    "    if not os.path.exists(config.SMILES_FILE):\n",
    "        logger.error(\"âŒ File SMILES non trovato.\"); sys.exit(1)\n",
    "    if not os.path.exists(config.SMARTS_RX_FILE):\n",
    "        logger.error(\"âŒ File SMARTS JSON non trovato.\"); sys.exit(1)\n",
    "\n",
    "    with open(config.SMILES_FILE) as f:\n",
    "        raw_smiles = [line.strip() for line in f if line.strip()]\n",
    "    logger.info(f\"âœ“ Caricati {len(raw_smiles)} SMILES.\")\n",
    "\n",
    "    # 2. Preprocessing\n",
    "    processed, vocab, max_len = process_dataset(raw_smiles)\n",
    "    config.MAX_LENGTH = max_len\n",
    "    logger.info(f\"âœ“ Vocab: {len(vocab)} token | MaxLen: {max_len}\")\n",
    "\n",
    "    # 3. Salvataggi dizionari\n",
    "    with open(\"vocab.json\", \"w\") as f: json.dump(vocab, f)\n",
    "    with open(\"char2idx.pkl\", \"wb\") as f: pickle.dump({c:i for i,c in enumerate(vocab)}, f)\n",
    "    with open(\"idx2char.pkl\", \"wb\") as f: pickle.dump({i:c for i,c in enumerate(vocab)}, f)\n",
    "   \n",
    "    # 4. Split\n",
    "    stratify = [min(len(t[1]), 20) for t in processed]  # t[1] = target\n",
    "    try:\n",
    "        train_data, val_data = train_test_split(processed, test_size=config.VALID_RATIO, stratify=stratify, random_state=42)\n",
    "    except:\n",
    "        train_data, val_data = train_test_split(processed, test_size=config.VALID_RATIO, random_state=42)\n",
    "\n",
    "    # 5. Esempio di Log\n",
    "    if train_data:\n",
    "        s_smart, s_targ = train_data[0]\n",
    "        logger.info(f\"\\n--- ESEMPIO SEQ ---\")\n",
    "        logger.info(f\"SMARTS:   {s_smart}\")\n",
    "        logger.info(f\"TARGET:   {''.join(s_targ)}\")\n",
    "        logger.info(\"-------------------\")\n",
    "\n",
    "    # 6. Avvio training\n",
    "    train_gen = CurriculumSmilesGenerator(train_data, vocab)\n",
    "    val_gen = CurriculumSmilesGenerator(val_data, vocab)\n",
    "     # 1. Salva train_data\n",
    "    with open(\"/home/grad/Desktop/pietro/denovo/2/smartrx/train_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    print(f\"âœ“ train_data salvato: {len(train_data)} molecole\")\n",
    "# 2. Salva val_data\n",
    "    with open(\"/home/grad/Desktop/pietro/denovo/2/smartrx/val_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(val_data, f)\n",
    "    print(f\"âœ“ val_data salvato: {len(val_data)} molecole\")\n",
    "    model = build_improved_model(len(vocab))\n",
    "    model.summary()\n",
    "    \n",
    "    callbacks = [\n",
    "        CustomTensorBoard(log_dir=f\"logs/smarts2smiles_{int(time.time())}\"),\n",
    "        EnhancedTrainingMonitor(val_gen),\n",
    "        ModelCheckpoint(\"best_smarts2smiles_model.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        model.fit(\n",
    "            train_gen.get_dataset(),\n",
    "            epochs=config.EPOCHS,\n",
    "            steps_per_epoch=config.STEPS_PER_EPOCH,\n",
    "            validation_data=val_gen.get_dataset(),\n",
    "            validation_steps=max(1, len(val_data)//config.BATCH_SIZE),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        model.save(\"final_smarts2smiles_model.keras\")\n",
    "        logger.info(\"âœ… Training Completato.\")\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"âš ï¸  Training interrotto.\")\n",
    "        model.save(\"interrupted_smarts2smiles.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad1056d-97c3-446f-b592-a102287a27eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 15:33:24,845 [INFO] ğŸš€ AVVIO SCRIPT: CARICAMENTO DIRETTO E ADDESTRAMENTO\n",
      "2026-01-12 15:33:24,846 [INFO] ğŸ“‚ Caricamento dati da disco (Pickle)...\n",
      "2026-01-12 15:33:29,620 [INFO] âœ… Dati caricati: 801613 Train, 89069 Val, 349 Vocab\n",
      "2026-01-12 15:33:29,621 [INFO] âš™ï¸ Inizializzazione Generatori...\n",
      "2026-01-12 15:35:57,340 [INFO] ğŸ—ï¸ Costruzione Modello...\n",
      "I0000 00:00:1768228557.449495   98451 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9090 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">178,688</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dynamic_positional_encoding     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DynamicPositionalEncoding</span>)     â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,402,944</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_1    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,402,944</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_2    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,402,944</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_3    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,402,944</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_4    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,402,944</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_5    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,402,944</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImprovedTransformerBlock</span>)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">349</span>)       â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">179,037</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚       \u001b[38;5;34m178,688\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dynamic_positional_encoding     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mDynamicPositionalEncoding\u001b[0m)     â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚     \u001b[38;5;34m8,402,944\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_1    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚     \u001b[38;5;34m8,402,944\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_2    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚     \u001b[38;5;34m8,402,944\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_3    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚     \u001b[38;5;34m8,402,944\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_4    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚     \u001b[38;5;34m8,402,944\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ improved_transformer_block_5    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚     \u001b[38;5;34m8,402,944\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mImprovedTransformerBlock\u001b[0m)      â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m349\u001b[0m)       â”‚       \u001b[38;5;34m179,037\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,775,389</span> (193.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,775,389\u001b[0m (193.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,775,389</span> (193.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,775,389\u001b[0m (193.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 15:35:59,277 [INFO] ğŸ”¥ Inizio Training Loop...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768228571.263623  102593 service.cc:152] XLA service 0x778034001fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1768228571.263645  102593 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A2000 12GB, Compute Capability 8.6\n",
      "2026-01-12 15:36:12.381283: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1768228575.965861  102593 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2026-01-12 15:36:19.663566: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 296 bytes spill stores, 296 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:20.332995: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 988 bytes spill stores, 884 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:20.929550: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:21.497429: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 944 bytes spill stores, 876 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:22.077318: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 572 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:22.674642: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:22.748791: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 920 bytes spill stores, 872 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:22.895844: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:23.364346: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:24.055719: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 348 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:24.313495: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 676 bytes spill stores, 560 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:24.443129: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 460 bytes spill stores, 916 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:24.466665: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:24.754893: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:26.039261: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_52', 456 bytes spill stores, 912 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:26.398819: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:26.450082: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 228 bytes spill stores, 180 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:28.115004: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 1208 bytes spill stores, 1208 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:28.169574: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 672 bytes spill stores, 648 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:28.372650: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 156 bytes spill stores, 156 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:29.064142: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 224 bytes spill stores, 224 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:29.116281: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 104 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:29.252430: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:29.564651: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_167', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:30.667555: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 444 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:30.851344: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 4048 bytes spill stores, 4032 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:31.089899: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:31.117973: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 4600 bytes spill stores, 4520 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:31.157115: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:31.563520: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:32.002035: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:32.387956: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:32.652244: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 420 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:33.750438: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:34.613886: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 1764 bytes spill stores, 1736 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:35.519076: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 540 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:35.621376: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:35.631655: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 1740 bytes spill stores, 1740 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:35.835143: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 844 bytes spill stores, 844 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:35.882911: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:35.931626: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 600 bytes spill stores, 600 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:36.346083: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:37.034652: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 452 bytes spill stores, 452 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:37.058900: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 484 bytes spill stores, 484 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:37.378666: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:37.702410: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 1740 bytes spill stores, 1740 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:37.936607: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:38.276223: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:38.397759: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:38.624926: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 432 bytes spill stores, 432 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:38.707240: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:39.238951: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:39.473077: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:39.723814: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 832 bytes spill stores, 832 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:39.811694: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 832 bytes spill stores, 832 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:39.955345: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 512 bytes spill stores, 512 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:40.297015: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 512 bytes spill stores, 512 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:40.341799: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:40.732132: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 440 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:40.758401: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:40.886712: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 628 bytes spill stores, 628 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:41.229395: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 440 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:41.285210: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:41.919472: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11911', 628 bytes spill stores, 628 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:42.109579: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 452 bytes spill stores, 452 bytes spill loads\n",
      "\n",
      "2026-01-12 15:36:42.152422: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12068', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "I0000 00:00:1768228615.115448  102593 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 501ms/step - loss: 4.2046 - val_loss: 2.7298 - lr: 4.4194e-05\n",
      "Epoch 2/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 504ms/step - loss: 2.6270 - val_loss: 2.1935 - lr: 8.8388e-05\n",
      "Epoch 3/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 503ms/step - loss: 2.1155 - val_loss: 1.7715 - lr: 1.3258e-04\n",
      "Epoch 4/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 503ms/step - loss: 1.7140 - val_loss: 1.4489 - lr: 1.7678e-04\n",
      "Epoch 5/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 509ms/step - loss: 1.3989 - val_loss: 1.2096 - lr: 2.2097e-04\n",
      "Epoch 6/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 1.1827 - val_loss: 1.0428 - lr: 2.6517e-04\n",
      "Epoch 7/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 1.0467 - val_loss: 0.9731 - lr: 3.0936e-04\n",
      "Epoch 8/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.9732 - val_loss: 0.9217 - lr: 3.5355e-04\n",
      "Epoch 9/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 503ms/step - loss: 0.9320 - val_loss: 0.9186 - lr: 3.9775e-04\n",
      "Epoch 10/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 502ms/step - loss: 0.9130 - val_loss: 0.8863 - lr: 4.4194e-04\n",
      "Epoch 11/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 509ms/step - loss: 0.8957 - val_loss: 0.8622 - lr: 4.2137e-04\n",
      "Epoch 12/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.8774 - val_loss: 0.8453 - lr: 4.0344e-04\n",
      "Epoch 13/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.8631 - val_loss: 0.8405 - lr: 3.8761e-04\n",
      "Epoch 14/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.8521 - val_loss: 0.8277 - lr: 3.7351e-04\n",
      "Epoch 15/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 493ms/step - loss: 0.8409 - val_loss: 0.8281 - lr: 3.6084e-04\n",
      "Epoch 16/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.8363 - val_loss: 0.8176 - lr: 3.4939e-04\n",
      "Epoch 17/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.8266 - val_loss: 0.8078 - lr: 3.3895e-04\n",
      "Epoch 18/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.8201 - val_loss: 0.8007 - lr: 3.2940e-04\n",
      "Epoch 19/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 493ms/step - loss: 0.8152 - val_loss: 0.8038 - lr: 3.2062e-04\n",
      "Epoch 20/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.8125 - val_loss: 0.7902 - lr: 3.1250e-04\n",
      "Epoch 21/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.8041 - val_loss: 0.7866 - lr: 3.0497e-04\n",
      "Epoch 22/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.8007 - val_loss: 0.7860 - lr: 2.9796e-04\n",
      "Epoch 23/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 497ms/step - loss: 0.7986 - val_loss: 0.7792 - lr: 2.9141e-04\n",
      "Epoch 24/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 493ms/step - loss: 0.7942 - val_loss: 0.7793 - lr: 2.8527e-04\n",
      "Epoch 25/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.7906 - val_loss: 0.7710 - lr: 2.7951e-04\n",
      "Epoch 26/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 497ms/step - loss: 0.7869 - val_loss: 0.7695 - lr: 2.7408e-04\n",
      "Epoch 27/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.7814 - val_loss: 0.7674 - lr: 2.6896e-04\n",
      "Epoch 28/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.7829 - val_loss: 0.7631 - lr: 2.6411e-04\n",
      "Epoch 29/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7774 - val_loss: 0.7620 - lr: 2.5952e-04\n",
      "Epoch 30/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.7734 - val_loss: 0.7592 - lr: 2.5516e-04\n",
      "Epoch 31/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.7725 - val_loss: 0.7554 - lr: 2.5101e-04\n",
      "Epoch 32/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.7686 - val_loss: 0.7535 - lr: 2.4705e-04\n",
      "Epoch 33/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.7671 - val_loss: 0.7505 - lr: 2.4328e-04\n",
      "Epoch 34/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.7628 - val_loss: 0.7494 - lr: 2.3968e-04\n",
      "Epoch 35/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7614 - val_loss: 0.7448 - lr: 2.3623e-04\n",
      "Epoch 36/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 494ms/step - loss: 0.7585 - val_loss: 0.7456 - lr: 2.3292e-04\n",
      "Epoch 37/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7593 - val_loss: 0.7423 - lr: 2.2975e-04\n",
      "Epoch 38/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 494ms/step - loss: 0.7570 - val_loss: 0.7429 - lr: 2.2671e-04\n",
      "Epoch 39/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7564 - val_loss: 0.7366 - lr: 2.2379e-04\n",
      "Epoch 40/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 494ms/step - loss: 0.7516 - val_loss: 0.7386 - lr: 2.2097e-04\n",
      "Epoch 41/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 498ms/step - loss: 0.7544 - val_loss: 0.7353 - lr: 2.1826e-04\n",
      "Epoch 42/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7483 - val_loss: 0.7350 - lr: 2.1565e-04\n",
      "Epoch 43/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7482 - val_loss: 0.7306 - lr: 2.1312e-04\n",
      "Epoch 44/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 494ms/step - loss: 0.7477 - val_loss: 0.7326 - lr: 2.1069e-04\n",
      "Epoch 45/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7442 - val_loss: 0.7297 - lr: 2.0833e-04\n",
      "Epoch 46/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7455 - val_loss: 0.7278 - lr: 2.0606e-04\n",
      "Epoch 47/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7432 - val_loss: 0.7257 - lr: 2.0385e-04\n",
      "Epoch 48/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7422 - val_loss: 0.7245 - lr: 2.0172e-04\n",
      "Epoch 49/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7397 - val_loss: 0.7231 - lr: 1.9965e-04\n",
      "Epoch 50/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7385 - val_loss: 0.7208 - lr: 1.9764e-04\n",
      "Epoch 51/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7364 - val_loss: 0.7211 - lr: 1.9570e-04\n",
      "Epoch 52/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7358 - val_loss: 0.7177 - lr: 1.9380e-04\n",
      "Epoch 53/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7355 - val_loss: 0.7168 - lr: 1.9197e-04\n",
      "Epoch 54/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 494ms/step - loss: 0.7330 - val_loss: 0.7187 - lr: 1.9018e-04\n",
      "Epoch 55/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 494ms/step - loss: 0.7315 - val_loss: 0.7186 - lr: 1.8844e-04\n",
      "Epoch 56/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.7308 - val_loss: 0.7148 - lr: 1.8675e-04\n",
      "Epoch 57/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 498ms/step - loss: 0.7302 - val_loss: 0.7138 - lr: 1.8511e-04\n",
      "Epoch 58/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 498ms/step - loss: 0.7291 - val_loss: 0.7118 - lr: 1.8351e-04\n",
      "Epoch 59/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7271 - val_loss: 0.7101 - lr: 1.8194e-04\n",
      "Epoch 60/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7280 - val_loss: 0.7104 - lr: 1.8042e-04\n",
      "Epoch 61/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7288 - val_loss: 0.7082 - lr: 1.7894e-04\n",
      "Epoch 62/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7255 - val_loss: 0.7076 - lr: 1.7749e-04\n",
      "Epoch 63/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7254 - val_loss: 0.7090 - lr: 1.7607e-04\n",
      "Epoch 64/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7249 - val_loss: 0.7077 - lr: 1.7469e-04\n",
      "Epoch 65/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7212 - val_loss: 0.7061 - lr: 1.7334e-04\n",
      "Epoch 66/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7226 - val_loss: 0.7058 - lr: 1.7203e-04\n",
      "Epoch 67/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7205 - val_loss: 0.7038 - lr: 1.7074e-04\n",
      "Epoch 68/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7218 - val_loss: 0.7013 - lr: 1.6948e-04\n",
      "Epoch 69/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7178 - val_loss: 0.7030 - lr: 1.6824e-04\n",
      "Epoch 70/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7186 - val_loss: 0.7014 - lr: 1.6704e-04\n",
      "Epoch 71/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7177 - val_loss: 0.7016 - lr: 1.6586e-04\n",
      "Epoch 72/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7188 - val_loss: 0.7007 - lr: 1.6470e-04\n",
      "Epoch 73/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7161 - val_loss: 0.6968 - lr: 1.6357e-04\n",
      "Epoch 74/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7153 - val_loss: 0.6996 - lr: 1.6246e-04\n",
      "Epoch 75/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7149 - val_loss: 0.6963 - lr: 1.6137e-04\n",
      "Epoch 76/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 494ms/step - loss: 0.7158 - val_loss: 0.7002 - lr: 1.6031e-04\n",
      "Epoch 77/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7129 - val_loss: 0.6950 - lr: 1.5926e-04\n",
      "Epoch 78/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7135 - val_loss: 0.6945 - lr: 1.5824e-04\n",
      "Epoch 79/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7100 - val_loss: 0.6973 - lr: 1.5724e-04\n",
      "Epoch 80/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7115 - val_loss: 0.6935 - lr: 1.5625e-04\n",
      "Epoch 81/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 494ms/step - loss: 0.7098 - val_loss: 0.6936 - lr: 1.5528e-04\n",
      "Epoch 82/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7101 - val_loss: 0.6944 - lr: 1.5433e-04\n",
      "Epoch 83/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7091 - val_loss: 0.6920 - lr: 1.5340e-04\n",
      "Epoch 84/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7092 - val_loss: 0.6903 - lr: 1.5248e-04\n",
      "Epoch 85/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7069 - val_loss: 0.6896 - lr: 1.5158e-04\n",
      "Epoch 86/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7084 - val_loss: 0.6893 - lr: 1.5070e-04\n",
      "Epoch 87/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7075 - val_loss: 0.6897 - lr: 1.4983e-04\n",
      "Epoch 88/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7062 - val_loss: 0.6878 - lr: 1.4898e-04\n",
      "Epoch 89/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7062 - val_loss: 0.6910 - lr: 1.4814e-04\n",
      "Epoch 90/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7047 - val_loss: 0.6887 - lr: 1.4731e-04\n",
      "Epoch 91/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7042 - val_loss: 0.6862 - lr: 1.4650e-04\n",
      "Epoch 92/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7032 - val_loss: 0.6857 - lr: 1.4570e-04\n",
      "Epoch 93/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7034 - val_loss: 0.6869 - lr: 1.4492e-04\n",
      "Epoch 94/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7018 - val_loss: 0.6840 - lr: 1.4415e-04\n",
      "Epoch 95/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7009 - val_loss: 0.6848 - lr: 1.4338e-04\n",
      "Epoch 96/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7026 - val_loss: 0.6846 - lr: 1.4264e-04\n",
      "Epoch 97/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.7010 - val_loss: 0.6838 - lr: 1.4190e-04\n",
      "Epoch 98/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7002 - val_loss: 0.6852 - lr: 1.4117e-04\n",
      "Epoch 99/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 498ms/step - loss: 0.6993 - val_loss: 0.6823 - lr: 1.4046e-04\n",
      "Epoch 100/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step - loss: 0.7010  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05:25:33] SMILES Parse Error: extra close parentheses while parsing: CC1OC(=O)N2C1(C)CCCC1C2CC1(C)C)OC(=O)NC1=CC=CC=C1\n",
      "[05:25:33] SMILES Parse Error: Failed parsing SMILES 'CC1OC(=O)N2C1(C)CCCC1C2CC1(C)C)OC(=O)NC1=CC=CC=C1' for input: 'CC1OC(=O)N2C1(C)CCCC1C2CC1(C)C)OC(=O)NC1=CC=CC=C1'\n",
      "2026-01-13 05:25:58,808 [INFO] \n",
      "EPOCH 100: Validity 90.0% | Novelty 9/9\n",
      "2026-01-13 05:25:58,809 [INFO] Sample: O=P(O)(OC1=CC=C(OC2=CC=CC=C2)C=C1)OC(P(=O)(O)O)P(=O)(O)O\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 545ms/step - loss: 0.7010 - val_loss: 0.6812 - lr: 1.3975e-04\n",
      "Epoch 101/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 494ms/step - loss: 0.7000 - val_loss: 0.6827 - lr: 1.3906e-04\n",
      "Epoch 102/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7005 - val_loss: 0.6814 - lr: 1.3838e-04\n",
      "Epoch 103/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.7003 - val_loss: 0.6826 - lr: 1.3770e-04\n",
      "Epoch 104/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6978 - val_loss: 0.6799 - lr: 1.3704e-04\n",
      "Epoch 105/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6974 - val_loss: 0.6801 - lr: 1.3639e-04\n",
      "Epoch 106/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 494ms/step - loss: 0.6951 - val_loss: 0.6812 - lr: 1.3574e-04\n",
      "Epoch 107/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6959 - val_loss: 0.6782 - lr: 1.3511e-04\n",
      "Epoch 108/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6971 - val_loss: 0.6813 - lr: 1.3448e-04\n",
      "Epoch 109/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 499ms/step - loss: 0.6960 - val_loss: 0.6782 - lr: 1.3386e-04\n",
      "Epoch 110/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6934 - val_loss: 0.6789 - lr: 1.3325e-04\n",
      "Epoch 111/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6929 - val_loss: 0.6770 - lr: 1.3265e-04\n",
      "Epoch 112/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6957 - val_loss: 0.6790 - lr: 1.3206e-04\n",
      "Epoch 113/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6958 - val_loss: 0.6768 - lr: 1.3147e-04\n",
      "Epoch 114/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6928 - val_loss: 0.6777 - lr: 1.3089e-04\n",
      "Epoch 115/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6928 - val_loss: 0.6773 - lr: 1.3032e-04\n",
      "Epoch 116/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 499ms/step - loss: 0.6903 - val_loss: 0.6754 - lr: 1.2976e-04\n",
      "Epoch 117/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6929 - val_loss: 0.6740 - lr: 1.2920e-04\n",
      "Epoch 118/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 496ms/step - loss: 0.6924 - val_loss: 0.6769 - lr: 1.2865e-04\n",
      "Epoch 119/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6928 - val_loss: 0.6752 - lr: 1.2811e-04\n",
      "Epoch 120/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6915 - val_loss: 0.6740 - lr: 1.2758e-04\n",
      "Epoch 121/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6915 - val_loss: 0.6731 - lr: 1.2705e-04\n",
      "Epoch 122/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6898 - val_loss: 0.6735 - lr: 1.2653e-04\n",
      "Epoch 123/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6888 - val_loss: 0.6736 - lr: 1.2601e-04\n",
      "Epoch 124/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6895 - val_loss: 0.6717 - lr: 1.2550e-04\n",
      "Epoch 125/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6894 - val_loss: 0.6719 - lr: 1.2500e-04\n",
      "Epoch 126/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6892 - val_loss: 0.6716 - lr: 1.2450e-04\n",
      "Epoch 127/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 496ms/step - loss: 0.6875 - val_loss: 0.6720 - lr: 1.2401e-04\n",
      "Epoch 128/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6884 - val_loss: 0.6714 - lr: 1.2353e-04\n",
      "Epoch 129/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6879 - val_loss: 0.6719 - lr: 1.2305e-04\n",
      "Epoch 130/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6880 - val_loss: 0.6706 - lr: 1.2257e-04\n",
      "Epoch 131/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6875 - val_loss: 0.6708 - lr: 1.2210e-04\n",
      "Epoch 132/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6861 - val_loss: 0.6710 - lr: 1.2164e-04\n",
      "Epoch 133/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6865 - val_loss: 0.6689 - lr: 1.2118e-04\n",
      "Epoch 134/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6856 - val_loss: 0.6688 - lr: 1.2073e-04\n",
      "Epoch 135/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6856 - val_loss: 0.6689 - lr: 1.2028e-04\n",
      "Epoch 136/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6867 - val_loss: 0.6688 - lr: 1.1984e-04\n",
      "Epoch 137/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6840 - val_loss: 0.6683 - lr: 1.1940e-04\n",
      "Epoch 138/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6854 - val_loss: 0.6698 - lr: 1.1897e-04\n",
      "Epoch 139/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6833 - val_loss: 0.6672 - lr: 1.1854e-04\n",
      "Epoch 140/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 502ms/step - loss: 0.6834 - val_loss: 0.6659 - lr: 1.1811e-04\n",
      "Epoch 141/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6840 - val_loss: 0.6666 - lr: 1.1769e-04\n",
      "Epoch 142/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6838 - val_loss: 0.6665 - lr: 1.1728e-04\n",
      "Epoch 143/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 505ms/step - loss: 0.6835 - val_loss: 0.6656 - lr: 1.1687e-04\n",
      "Epoch 144/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 500ms/step - loss: 0.6834 - val_loss: 0.6669 - lr: 1.1646e-04\n",
      "Epoch 145/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 504ms/step - loss: 0.6833 - val_loss: 0.6647 - lr: 1.1606e-04\n",
      "Epoch 146/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 504ms/step - loss: 0.6828 - val_loss: 0.6642 - lr: 1.1566e-04\n",
      "Epoch 147/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 503ms/step - loss: 0.6833 - val_loss: 0.6635 - lr: 1.1527e-04\n",
      "Epoch 148/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 502ms/step - loss: 0.6816 - val_loss: 0.6638 - lr: 1.1488e-04\n",
      "Epoch 149/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 502ms/step - loss: 0.6824 - val_loss: 0.6642 - lr: 1.1449e-04\n",
      "Epoch 150/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 505ms/step - loss: 0.6815 - val_loss: 0.6632 - lr: 1.1411e-04\n",
      "Epoch 151/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 501ms/step - loss: 0.6808 - val_loss: 0.6638 - lr: 1.1373e-04\n",
      "Epoch 152/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 501ms/step - loss: 0.6814 - val_loss: 0.6637 - lr: 1.1336e-04\n",
      "Epoch 153/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 501ms/step - loss: 0.6809 - val_loss: 0.6639 - lr: 1.1298e-04\n",
      "Epoch 154/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 501ms/step - loss: 0.6795 - val_loss: 0.6638 - lr: 1.1262e-04\n",
      "Epoch 155/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 501ms/step - loss: 0.6814 - val_loss: 0.6633 - lr: 1.1225e-04\n",
      "Epoch 156/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m510s\u001b[0m 510ms/step - loss: 0.6806 - val_loss: 0.6633 - lr: 1.1189e-04\n",
      "Epoch 157/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 517ms/step - loss: 0.6785 - val_loss: 0.6618 - lr: 1.1154e-04\n",
      "Epoch 158/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6787 - val_loss: 0.6622 - lr: 1.1118e-04\n",
      "Epoch 159/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6787 - val_loss: 0.6621 - lr: 1.1083e-04\n",
      "Epoch 160/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 504ms/step - loss: 0.6785 - val_loss: 0.6612 - lr: 1.1049e-04\n",
      "Epoch 161/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 503ms/step - loss: 0.6793 - val_loss: 0.6613 - lr: 1.1014e-04\n",
      "Epoch 162/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 502ms/step - loss: 0.6798 - val_loss: 0.6636 - lr: 1.0980e-04\n",
      "Epoch 163/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 509ms/step - loss: 0.6786 - val_loss: 0.6612 - lr: 1.0946e-04\n",
      "Epoch 164/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6754 - val_loss: 0.6616 - lr: 1.0913e-04\n",
      "Epoch 165/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 501ms/step - loss: 0.6763 - val_loss: 0.6589 - lr: 1.0880e-04\n",
      "Epoch 166/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 500ms/step - loss: 0.6766 - val_loss: 0.6599 - lr: 1.0847e-04\n",
      "Epoch 167/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 506ms/step - loss: 0.6776 - val_loss: 0.6591 - lr: 1.0815e-04\n",
      "Epoch 168/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 498ms/step - loss: 0.6782 - val_loss: 0.6601 - lr: 1.0782e-04\n",
      "Epoch 169/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6771 - val_loss: 0.6597 - lr: 1.0750e-04\n",
      "Epoch 170/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.6765 - val_loss: 0.6596 - lr: 1.0719e-04\n",
      "Epoch 171/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.6765 - val_loss: 0.6598 - lr: 1.0687e-04\n",
      "Epoch 172/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6776 - val_loss: 0.6581 - lr: 1.0656e-04\n",
      "Epoch 173/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 499ms/step - loss: 0.6739 - val_loss: 0.6573 - lr: 1.0625e-04\n",
      "Epoch 174/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6759 - val_loss: 0.6578 - lr: 1.0595e-04\n",
      "Epoch 175/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6757 - val_loss: 0.6580 - lr: 1.0564e-04\n",
      "Epoch 176/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6742 - val_loss: 0.6584 - lr: 1.0534e-04\n",
      "Epoch 177/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6748 - val_loss: 0.6574 - lr: 1.0505e-04\n",
      "Epoch 178/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6737 - val_loss: 0.6561 - lr: 1.0475e-04\n",
      "Epoch 179/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6745 - val_loss: 0.6578 - lr: 1.0446e-04\n",
      "Epoch 180/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.6729 - val_loss: 0.6563 - lr: 1.0417e-04\n",
      "Epoch 181/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 513ms/step - loss: 0.6756 - val_loss: 0.6552 - lr: 1.0388e-04\n",
      "Epoch 182/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6743 - val_loss: 0.6555 - lr: 1.0359e-04\n",
      "Epoch 183/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6716 - val_loss: 0.6553 - lr: 1.0331e-04\n",
      "Epoch 184/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 497ms/step - loss: 0.6736 - val_loss: 0.6563 - lr: 1.0303e-04\n",
      "Epoch 185/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.6730 - val_loss: 0.6573 - lr: 1.0275e-04\n",
      "Epoch 186/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6712 - val_loss: 0.6555 - lr: 1.0247e-04\n",
      "Epoch 187/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6713 - val_loss: 0.6551 - lr: 1.0220e-04\n",
      "Epoch 188/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 500ms/step - loss: 0.6724 - val_loss: 0.6545 - lr: 1.0193e-04\n",
      "Epoch 189/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.6721 - val_loss: 0.6556 - lr: 1.0166e-04\n",
      "Epoch 190/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6709 - val_loss: 0.6550 - lr: 1.0139e-04\n",
      "Epoch 191/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.6711 - val_loss: 0.6548 - lr: 1.0112e-04\n",
      "Epoch 192/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 501ms/step - loss: 0.6706 - val_loss: 0.6544 - lr: 1.0086e-04\n",
      "Epoch 193/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 497ms/step - loss: 0.6725 - val_loss: 0.6547 - lr: 1.0060e-04\n",
      "Epoch 194/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6695 - val_loss: 0.6534 - lr: 1.0034e-04\n",
      "Epoch 195/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6705 - val_loss: 0.6525 - lr: 1.0008e-04\n",
      "Epoch 196/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6724 - val_loss: 0.6540 - lr: 9.9824e-05\n",
      "Epoch 197/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6713 - val_loss: 0.6532 - lr: 9.9571e-05\n",
      "Epoch 198/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6696 - val_loss: 0.6533 - lr: 9.9319e-05\n",
      "Epoch 199/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 497ms/step - loss: 0.6705 - val_loss: 0.6530 - lr: 9.9069e-05\n",
      "Epoch 200/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371ms/step - loss: 0.6699  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 19:17:31,129 [INFO] \n",
      "EPOCH 200: Validity 100.0% | Novelty 10/10\n",
      "2026-01-13 19:17:31,130 [INFO] Sample: NCCC1=CC=CC2=CC=C1C=C2CCCOC1=CC=CC=C1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 537ms/step - loss: 0.6699 - val_loss: 0.6520 - lr: 9.8821e-05\n",
      "Epoch 201/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 495ms/step - loss: 0.6691 - val_loss: 0.6525 - lr: 9.8575e-05\n",
      "Epoch 202/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 500ms/step - loss: 0.6678 - val_loss: 0.6513 - lr: 9.8331e-05\n",
      "Epoch 203/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6685 - val_loss: 0.6536 - lr: 9.8088e-05\n",
      "Epoch 204/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6695 - val_loss: 0.6517 - lr: 9.7848e-05\n",
      "Epoch 205/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6702 - val_loss: 0.6500 - lr: 9.7609e-05\n",
      "Epoch 206/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6681 - val_loss: 0.6507 - lr: 9.7371e-05\n",
      "Epoch 207/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6671 - val_loss: 0.6520 - lr: 9.7136e-05\n",
      "Epoch 208/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6669 - val_loss: 0.6505 - lr: 9.6902e-05\n",
      "Epoch 209/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6687 - val_loss: 0.6494 - lr: 9.6670e-05\n",
      "Epoch 210/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6687 - val_loss: 0.6505 - lr: 9.6440e-05\n",
      "Epoch 211/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6680 - val_loss: 0.6502 - lr: 9.6211e-05\n",
      "Epoch 212/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6687 - val_loss: 0.6511 - lr: 9.5984e-05\n",
      "Epoch 213/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6660 - val_loss: 0.6518 - lr: 9.5758e-05\n",
      "Epoch 214/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6657 - val_loss: 0.6495 - lr: 9.5534e-05\n",
      "Epoch 215/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6654 - val_loss: 0.6504 - lr: 9.5312e-05\n",
      "Epoch 216/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6671 - val_loss: 0.6494 - lr: 9.5091e-05\n",
      "Epoch 217/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6679 - val_loss: 0.6485 - lr: 9.4871e-05\n",
      "Epoch 218/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6678 - val_loss: 0.6494 - lr: 9.4654e-05\n",
      "Epoch 219/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 497ms/step - loss: 0.6680 - val_loss: 0.6493 - lr: 9.4437e-05\n",
      "Epoch 220/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6668 - val_loss: 0.6484 - lr: 9.4222e-05\n",
      "Epoch 221/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6651 - val_loss: 0.6492 - lr: 9.4009e-05\n",
      "Epoch 222/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 496ms/step - loss: 0.6658 - val_loss: 0.6485 - lr: 9.3797e-05\n",
      "Epoch 223/400\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 499ms/step - loss: 0.6649 - val_loss: 0.6477 - lr: 9.3586e-05\n",
      "Epoch 224/400\n",
      "\u001b[1m 769/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m1:25\u001b[0m 371ms/step - loss: 0.6644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 22:32:44.564722: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: KeyError: '9'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/tmp/ipykernel_98451/3079994727.py\", line 271, in __next__\n",
      "    with self.lock: return next(self.iterator)\n",
      "\n",
      "  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in __call__\n",
      "    indices = [self.char2idx[t] for t in padded_seq]\n",
      "\n",
      "  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in <listcomp>\n",
      "    indices = [self.char2idx[t] for t in padded_seq]\n",
      "\n",
      "KeyError: '9'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 770/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m1:25\u001b[0m 371ms/step - loss: 0.6644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 22:32:44.930616: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: KeyError: '9'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/tmp/ipykernel_98451/3079994727.py\", line 271, in __next__\n",
      "    with self.lock: return next(self.iterator)\n",
      "\n",
      "  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in __call__\n",
      "    indices = [self.char2idx[t] for t in padded_seq]\n",
      "\n",
      "  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in <listcomp>\n",
      "    indices = [self.char2idx[t] for t in padded_seq]\n",
      "\n",
      "KeyError: '9'\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "2026-01-13 22:32:44.931118: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: KeyError: '9'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/tmp/ipykernel_98451/3079994727.py\", line 271, in __next__\n",
      "    with self.lock: return next(self.iterator)\n",
      "\n",
      "  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in __call__\n",
      "    indices = [self.char2idx[t] for t in padded_seq]\n",
      "\n",
      "  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in <listcomp>\n",
      "    indices = [self.char2idx[t] for t in padded_seq]\n",
      "\n",
      "KeyError: '9'\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "\t [[IteratorGetNext/_4]]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNKNOWN:  KeyError: '9'\nTraceback (most recent call last):\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 271, in __next__\n    with self.lock: return next(self.iterator)\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in __call__\n    indices = [self.char2idx[t] for t in padded_seq]\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in <listcomp>\n    indices = [self.char2idx[t] for t in padded_seq]\n\nKeyError: '9'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_4]]\n  (1) UNKNOWN:  KeyError: '9'\nTraceback (most recent call last):\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 271, in __next__\n    with self.lock: return next(self.iterator)\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in __call__\n    indices = [self.char2idx[t] for t in padded_seq]\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in <listcomp>\n    indices = [self.char2idx[t] for t in padded_seq]\n\nKeyError: '9'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_27475]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     36\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     37\u001b[0m     CustomTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/run_loaded_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     38\u001b[0m     EnhancedTrainingMonitor(val_gen),\n\u001b[1;32m     39\u001b[0m     ModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_hybrid_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m ]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_hybrid_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ† Training Completato.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNKNOWN:  KeyError: '9'\nTraceback (most recent call last):\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 271, in __next__\n    with self.lock: return next(self.iterator)\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in __call__\n    indices = [self.char2idx[t] for t in padded_seq]\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in <listcomp>\n    indices = [self.char2idx[t] for t in padded_seq]\n\nKeyError: '9'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_4]]\n  (1) UNKNOWN:  KeyError: '9'\nTraceback (most recent call last):\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 271, in __next__\n    with self.lock: return next(self.iterator)\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in __call__\n    indices = [self.char2idx[t] for t in padded_seq]\n\n  File \"/tmp/ipykernel_98451/3079994727.py\", line 403, in <listcomp>\n    indices = [self.char2idx[t] for t in padded_seq]\n\nKeyError: '9'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_27475]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"ğŸš€ AVVIO SCRIPT: CARICAMENTO DIRETTO E ADDESTRAMENTO\")\n",
    "    \n",
    "    # 1. CONTROLLO FILE ESISTENTI\n",
    "    if not (os.path.exists(config.TRAIN_DATA_PATH) and \n",
    "            os.path.exists(config.VAL_DATA_PATH) and \n",
    "            os.path.exists(config.VOCAB_PATH)):\n",
    "        logger.error(f\"âŒ File di cache mancanti! Esegui prima lo script di salvataggio.\")\n",
    "        logger.error(f\"   Cercati: {config.TRAIN_DATA_PATH}, {config.VAL_DATA_PATH}, {config.VOCAB_PATH}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 2. CARICAMENTO DATI\n",
    "    logger.info(\"ğŸ“‚ Caricamento dati da disco (Pickle)...\")\n",
    "    try:\n",
    "        with open(config.TRAIN_DATA_PATH, \"rb\") as f: train_data = pickle.load(f)\n",
    "        with open(config.VAL_DATA_PATH, \"rb\") as f: val_data = pickle.load(f)\n",
    "        with open(config.VOCAB_PATH, \"r\") as f: vocab = json.load(f)\n",
    "        logger.info(f\"âœ… Dati caricati: {len(train_data)} Train, {len(val_data)} Val, {len(vocab)} Vocab\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Errore durante il caricamento: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3. SETUP GENERATORI\n",
    "    logger.info(\"âš™ï¸ Inizializzazione Generatori...\")\n",
    "    train_gen = CurriculumSmilesGenerator(train_data, vocab)\n",
    "    val_gen = CurriculumSmilesGenerator(val_data, vocab)\n",
    "\n",
    "    # 4. SETUP MODELLO\n",
    "    logger.info(\"ğŸ—ï¸ Costruzione Modello...\")\n",
    "    model = build_improved_model(len(vocab))\n",
    "    model.summary()\n",
    "\n",
    "    # 5. AVVIO TRAINING\n",
    "    logger.info(\"ğŸ”¥ Inizio Training Loop...\")\n",
    "    \n",
    "    callbacks = [\n",
    "        CustomTensorBoard(log_dir=f\"logs/run_loaded_{int(time.time())}\"),\n",
    "        EnhancedTrainingMonitor(val_gen),\n",
    "        ModelCheckpoint(\"best_hybrid_model.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "    ]\n",
    "    try:\n",
    "        model.fit(\n",
    "            train_gen.get_dataset(),\n",
    "            epochs=config.EPOCHS,\n",
    "            steps_per_epoch=config.STEPS_PER_EPOCH,\n",
    "            validation_data=val_gen.get_dataset(),\n",
    "            validation_steps=max(1, len(val_data)//config.BATCH_SIZE),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        model.save(\"final_hybrid_model.keras\")\n",
    "        logger.info(\"ğŸ† Training Completato.\")\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"âš ï¸ Training interrotto dall'utente.\")\n",
    "        model.save(\"interrupted_hybrid.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85136e07-7225-4f1e-95b6-425e4c2ceafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
